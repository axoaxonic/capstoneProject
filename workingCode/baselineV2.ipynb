{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE MODEL\n",
    "-----------------\n",
    "-----------------\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Conv1D, GlobalMaxPooling2D, LSTM, MaxPooling2D, MaxPooling1D, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15Gi       1.9Gi        12Gi       202Mi       911Mi        13Gi\r\n",
      "Swap:          976Mi       671Mi       305Mi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This df will be a row-concatenated version of the same channels\n",
    "# baseline = pd.read_csv('../../baseline.csv', delimiter=',')\n",
    "# baseline.drop('Unnamed: 0', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/dfTensor.csv', delimiter=';', encoding='latin-1')\n",
    "df.drop('Unnamed: 0', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[3989.23077, 3983.589744, 3987.692308, 3991.794872, 3992.307693, 3990.256411, 3993.333334, 4000.0, 3994.871795, 3985.128206, 3995.897436, 4001.538461, 3986.153847, 3982.051283, 3990.256411, 3994.358975, 3981.538462, 3961.025642, 3968.205129, 3989.23077, 3980.512821, 3973.333334, 3997.948718, 4009.230769, 3998.974359, 3998.461539, 3999.48718, 3997.435898, 3999.48718, 4001.025641, 3994.871795, 3987.179488, 3987.692308, 3992.307693, 3987.692308, 3980.0, 3984.102565, 3994.358975, 3991.794872, 3982.564103, 3980.0, 3978.461539, 3978.461539, 3989.23077, 3998.461539, 3998.461539, 3996.410257, 3978.974359, 3966.153847, 3978.974359, 3987.692308, 3985.128206, 3993.846154, 3995.384616, 3974.871795, 3957.948718, 3965.128206, 3990.769231, 4007.179487, 3998.974359, 3981.025642, 3970.256411, 3976.923077, 3993.333334, 3992.820513, 3978.461539, 3976.923077, 3975.897436, 3973.846154, 3984.615385, 3986.153847, 3978.974359, 3980.0, 3982.051283, 3981.538462, 3983.589744, 3978.974359, 3971.282052, 3972.820513, 3971.794872, 3969.23077, 3976.923077, 3975.897436, 3967.692308, 3972.820513, 3984.102565, 3983.589744, 3974.871795, 3967.692308, 3966.153847, 3974.358975, 3981.025642, 3971.282052, 3963.076924, 3973.846154, 3986.153847, 3983.076924, 3981.025642, 3986.666667, 3984.615385, 3977.435898, 3975.384616, 3974.358975, 3973.333334, 3978.461539, 3983.589744, 3984.615385, 3990.256411, 3988.205129, 3974.358975, 3983.589744, 4000.51282, 3986.153847, 3969.23077, 3979.48718, 3987.692308, 3980.512821, 3980.512821, 3994.358975, 4001.538461, 3988.717949, 3981.025642, 3992.820513, 3996.923077, 3988.717949, 3981.538462, 3973.846154, 3970.769231, 3982.051283, 3998.461539, 3999.48718, 3989.74359, 3984.615385, 3989.23077, 3988.205129, 3971.282052, 3969.74359, 3990.256411, 3990.769231, 3967.692308, 3963.076924, 3978.974359, 3987.692308, 3981.538462, 3974.871795, 3986.666667, 3994.871795, 3980.512821, 3975.384616, 3986.153847, 3984.615385, 3980.512821, 3985.641026, 3986.153847, 3980.512821, 3986.153847, 3991.794872, 3978.461539, 3970.256411, 3985.128206, 3989.23077, 3982.051283, 3987.692308, 3990.769231, 3982.564103, 3989.74359, 4000.0, 3985.641026, 3973.333334, 3989.74359, 4012.307692, 4016.923076, 4009.743589, 4008.717948, 4010.76923, 4005.641025, 3998.461539, 3997.435898, 3997.948718, 3994.871795, 3992.307693, 3991.282052, 3989.23077, 3990.256411, 3994.358975, 3997.948718, 4003.589743, 4008.717948, 4008.717948, 3998.974359, 3985.128206, 3985.128206, 3996.923077, 3999.48718, 3990.256411, 3983.589744, 3982.564103, 3981.025642, 3981.025642, 3988.717949, 3990.769231, 3986.666667, 3996.410257, 4000.51282, 3987.692308, 3982.564103, 3986.153847, 3990.769231, 4001.538461, 4006.153846, 3994.358975, 3982.051283, 3986.153847, 4000.0, 3999.48718, 3986.666667, 3985.128206, 3991.794872, 3996.923077, 4004.102564, 3997.948718, 3985.128206, 3994.871795, 4007.179487, 3995.897436, 3991.282052, 4007.692307, 4013.846153, 4003.076923, 3994.871795, 3997.948718, 4008.717948, 4013.846153, 4005.641025, 3995.384616, 3993.846154, 3999.48718, 4006.666666, 4008.717948, 4004.615384, 3999.48718, 3993.333334, 3993.333334, 4000.51282, 4001.538461, 3997.948718, 3999.48718, 4001.025641, 3996.410257, 3996.923077, 3997.435898, 3994.358975, 3996.410257, 4001.025641, 4005.641025, 4014.871794]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['F8'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['O1'] = df['O1'].apply(eval)\n",
    "# df['O1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataframes for each channel, pick and choose at will\n",
    "\n",
    "# occ0 = pd.read_csv('../data/occ0Exp.csv', delimiter=',')\n",
    "# occ1 = pd.read_csv('../data/occ1Exp.csv', delimiter=',')\n",
    "# fefF3 = pd.read_csv('../data/fefF3Exp.csv', delimiter=',')\n",
    "# fefF4 = pd.read_csv('../data/fefF4Exp.csv', delimiter=',')\n",
    "# fefF7 = pd.read_csv('../data/fefF7Exp.csv', delimiter=',')\n",
    "# fefF8 = pd.read_csv('../data/fefF8Exp.csv', delimiter=',')\n",
    "# temT7 = pd.read_csv('../data/temT7Exp.csv', delimiter=',')\n",
    "# temT8 = pd.read_csv('../data/temT8Exp.csv', delimiter=',')\n",
    "# pfcAF3 = pd.read_csv('../data/pfcAF3Exp.csv', delimiter=',')\n",
    "# pfcAF4 = pd.read_csv('../data/pfcAF4Exp.csv', delimiter=',')\n",
    "# motFC5 = pd.read_csv('../data/motFC5Exp.csv', delimiter=',')\n",
    "# motFC6 = pd.read_csv('../data/motFC6Exp.csv', delimiter=',')\n",
    "# parP7 = pd.read_csv('../data/parP7Exp.csv', delimiter=',')\n",
    "# parP8 = pd.read_csv('../data/parP8Exp.csv', delimiter=',')\n",
    "\n",
    "# loading same datasets expanded with filter processing\n",
    "\n",
    "# occ0 = pd.read_csv('../data/occ0Proc.csv', delimiter=',')\n",
    "# occ1 = pd.read_csv('../data/occ1Proc.csv', delimiter=',') \n",
    "# fefF3 = pd.read_csv('../data/fefF3Proc.csv', delimiter=',')\n",
    "# fefF4 = pd.read_csv('../data/fefF4Proc.csv', delimiter=',')\n",
    "# fefF7 = pd.read_csv('../data/fefF7Proc.csv', delimiter=',')\n",
    "# fefF8 = pd.read_csv('../data/fefF8Proc.csv', delimiter=',') \n",
    "# temT7 = pd.read_csv('../data/temT7Proc.csv', delimiter=',')\n",
    "# temT8 = pd.read_csv('../data/temT8Proc.csv', delimiter=',')\n",
    "# pfcAF3 = pd.read_csv('../data/pfcAF3Proc.csv', delimiter=',')\n",
    "# pfcAF4 = pd.read_csv('../data/pfcAF4Proc.csv', delimiter=',')\n",
    "# motFC5 = pd.read_csv('../data/motFC5Proc.csv', delimiter=',')\n",
    "# motFC6 = pd.read_csv('../data/motFC6Proc.csv', delimiter=',')\n",
    "# parP7 = pd.read_csv('../data/parP7Proc.csv', delimiter=',')\n",
    "# parP8 = pd.read_csv('../data/parP8Proc.csv', delimiter=',')\n",
    "\n",
    "# occ0 = pd.read_csv('../data/occ0Wide.csv', delimiter=',')\n",
    "# occ1 = pd.read_csv('../data/occ1Wide.csv', delimiter=',') \n",
    "# fefF3 = pd.read_csv('../data/fefF3Wide.csv', delimiter=',')\n",
    "# fefF4 = pd.read_csv('../data/fefF4Wide.csv', delimiter=',')\n",
    "# fefF7 = pd.read_csv('../data/fefF7Wide.csv', delimiter=',')\n",
    "# fefF8 = pd.read_csv('../data/fefF8Wide.csv', delimiter=',') \n",
    "# temT7 = pd.read_csv('../data/temT7Wide.csv', delimiter=',')\n",
    "# temT8 = pd.read_csv('../data/temT8Wide.csv', delimiter=',')\n",
    "# pfcAF3 = pd.read_csv('../data/pfcAF3Wide.csv', delimiter=',')\n",
    "# pfcAF4 = pd.read_csv('../data/pfcAF4Wide.csv', delimiter=',')\n",
    "# motFC5 = pd.read_csv('../data/motFC5Wide.csv', delimiter=',')\n",
    "# motFC6 = pd.read_csv('../data/motFC6Wide.csv', delimiter=',')\n",
    "# parP7 = pd.read_csv('../data/parP7Wide.csv', delimiter=',')\n",
    "# parP8 = pd.read_csv('../data/parP8Wide.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del occ0\n",
    "# del occ1\n",
    "# del fefF3\n",
    "# del fefF4\n",
    "# del fefF7\n",
    "# del fefF8\n",
    "# del temT7\n",
    "# del temT8\n",
    "# del pfcAF3\n",
    "# del pfcAF4\n",
    "# del motFC5\n",
    "# del motFC6\n",
    "# del parP7\n",
    "# del parP8\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading only the dataframes from channels positively correlated with code, to save RAM\n",
    "# corrcols = [occ1, fefF4, fefF8, pfcAF4, motFC6, parP8] #removed motFC5 for being a diff shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations of averaged channels, if there is a need to pick and choose to save RAM:\n",
    "\n",
    "<sub>\n",
    "code          1.000000\n",
    "    \n",
    "&nbsp;\n",
    "    \n",
    "occ0Data     -0.000385\n",
    "\n",
    "&nbsp;\n",
    "    \n",
    "occ1Data      0.001626\n",
    "    \n",
    "&nbsp;\n",
    "\n",
    "fefF3Data    -0.005483\n",
    "\n",
    "&nbsp;\n",
    "    \n",
    "fefF4Data     0.008288\n",
    "\n",
    "&nbsp;\n",
    "    \n",
    "fefF7Data    -0.004799\n",
    "\n",
    "&nbsp;\n",
    "        \n",
    "fefF8Data     0.006907\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "temT7Data    -0.002657\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "temT8Data    -0.001037\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "pfcAF3Data   -0.007512\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "pfcAF4Data    0.007591\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "motFC5Data    0.003757\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "motFC6Data    0.001372\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "parP7Data    -0.003000\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "parP8Data     0.002366\n",
    "</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15Gi       6.1Gi       1.8Gi       438Mi       7.6Gi       8.6Gi\r\n",
      "Swap:          976Mi       352Mi       624Mi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If individual columns are loaded, use this cell to process them and aggregate group them by event\n",
    "# This gets rid of the time dimension though and will make the model be highly inaccurate\n",
    "\n",
    "# dfs = [occ0, occ1, fefF3, fefF4, fefF7, fefF8, \n",
    "#        temT7, temT8, pfcAF3, pfcAF4, motFC5, motFC6, parP7, parP8]\n",
    "# for i in dfs: #dfs:\n",
    "# #     i.drop('Unnamed: 0.1', inplace=True, axis=1)\n",
    "#     i.drop(['Unnamed: 0', 'size'], inplace=True, axis=1)\n",
    "#     i['data'].astype(float, copy=False)\n",
    "#     print(i.columns)\n",
    "# #     i = i.groupby('event').mean()\n",
    "# #     print(i.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non loop version of above's last line\n",
    "# occ1, fefF4, fefF8, pfcAF4, motFC5, motFC6, parP8 for corrcols\n",
    "\n",
    "# occ0 = occ0.groupby('event').mean()\n",
    "# occ1 = occ1.groupby('event').mean()\n",
    "# fefF3 = fefF3.groupby('event').mean()\n",
    "# fefF4 = fefF4.groupby('event').mean()\n",
    "# fefF7 = fefF7.groupby('event').mean()\n",
    "# fefF8 = fefF8.groupby('event').mean()\n",
    "# temT7 = temT7.groupby('event').mean()\n",
    "# temT8 = temT8.groupby('event').mean()\n",
    "# pfcAF3 = pfcAF3.groupby('event').mean()\n",
    "# pfcAF4 = pfcAF4.groupby('event').mean()\n",
    "# motFC5 = motFC5.groupby('event').mean() #this one has less rows than the rest, why?\n",
    "# motFC6 = motFC6.groupby('event').mean()\n",
    "# parP7 = parP7.groupby('event').mean()\n",
    "# parP8 = parP8.groupby('event').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Check each channel dataframe for null vals \n",
    "# for i in corrcols:\n",
    "#     print(max(i.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Renaming data column to specify channel name in individual channel dataframes, so when they are concatenated, the full df doesn't have multiple cols with the same names\n",
    "## (use  occ1, fefF4, fefF8, pfcAF4, motFC6, parP8 for corrcols)\n",
    "\n",
    "# occ0.rename(columns={'data': 'occ0data'}, inplace=True)\n",
    "# occ1.rename(columns={'data':'occ1data', 'ICAdata':'occ1ICAdata', 'SGFdata':'occ1SGFdata'}, inplace=True)\n",
    "# fefF3.rename(columns={'data': 'fefF3data'}, inplace=True)\n",
    "# fefF4.rename(columns={'data':'fefF4data', 'ICAdata':'fefF4ICAdata', 'SGFdata':'fefF4SGFdata'}, inplace=True)\n",
    "# fefF7.rename(columns={'data': 'fefF7data'}, inplace=True)\n",
    "# fefF8.rename(columns={'data':'fefF8data', 'ICAdata':'fefF8ICAdata', 'SGFdata':'fefF8SGFdata'}, inplace=True)\n",
    "# temT7.rename(columns={'data': 'temT7data'}, inplace=True)\n",
    "# temT8.rename(columns={'data': 'temT8data'}, inplace=True)\n",
    "# pfcAF3.rename(columns={'data': 'pfcAF3data'}, inplace=True)\n",
    "# pfcAF4.rename(columns={'data':'pfcAF4data', 'ICAdata':'pfcAF4ICAdata', 'SGFdata':'pfcAF4SGFdata'}, inplace=True)\n",
    "# motFC5.rename(columns={'data': 'motFC5data'}, inplace=True)\n",
    "# motFC6.rename(columns={'data':'motFC6data', 'ICAdata':'motFC6ICAdata', 'SGFdata':'motFC6SGFdata'}, inplace=True)\n",
    "# parP7.rename(columns={'data': 'parP7data'}, inplace=True)\n",
    "# parP8.rename(columns={'data':'parP8data', 'ICAdata':'parP8ICAdata', 'SGFdata':'parP8SGFdata'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occ0.columns, occ1.columns, fefF7.columns, fefF8.columns\n",
    "# # dfs[0] #this wasn't changed with the above functions, it's only used for calling the original datasets as they were read in\n",
    "# dfsProcessed = [occ0, occ1, fefF3, fefF4, fefF7, fefF8, \n",
    "#        temT7, temT8, pfcAF3, pfcAF4, motFC5, motFC6, parP7, parP8]\n",
    "\n",
    "## Remaking this list included processed dataframes\n",
    "# corrcolsProcessed = [occ1, fefF4, fefF8, pfcAF4, motFC6, parP8] #new vars because the original list has the unprocessed dataframes in memory it seems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If using individual channel dataframes, use this cell to turn them into one df\n",
    "# full = pd.concat(corrcolsProcessed, axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full.columns.duplicated()\n",
    "# # Since the concat created duplicate code columns, using this trick to delete them:\n",
    "# #https://stackoverflow.com/questions/14984119/python-pandas-remove-duplicate-columns\n",
    "# #Although a different type of join might prevent the need for this\n",
    "# full = full.loc[:,~full.columns.duplicated()].copy()\n",
    "# #full = full.drop(['size', 'id' ], axis=1) # If any extraneous cols are leftover\n",
    "# full.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrMatFull = full.corr()\n",
    "#corrMatFull['code']\n",
    "# check = pd.read_csv('../data/occ1Exp.csv', delimiter=',')\n",
    "# print(check.corr()) # code correlates to data in this channel by 0.001562 \n",
    "# del check \n",
    "\n",
    "# check = pd.read_csv('../data/fefF7Exp.csv', delimiter=',')\n",
    "# print(check.corr()) # code correlates to data in this channel by -0.004390.\n",
    "# del check \n",
    "\n",
    "# check = pd.read_csv('../data/occ0Exp.csv', delimiter=',')\n",
    "# print(check.corr()) # code correlates to data in this channel by -0.000308 \n",
    "# del check \n",
    "\n",
    "# check = pd.read_csv('../data/fefF8Exp.csv', delimiter=',')\n",
    "# print(check.corr()) # code correlates to data in this channel by 0.006184\n",
    "# del check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# full.plot(y=['occ0Data', 'occ1Data', 'fefF7Data', 'fefF8Data'], alpha=.7 ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15Gi       8.8Gi       4.4Gi       182Mi       2.3Gi       6.3Gi\r\n",
      "Swap:          976Mi       381Mi       595Mi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Baseline Model: Only O1, O2, F7, F8; No Preprocessing\n",
    "## Baseline+ : Basline+Preprocessing\n",
    "\n",
    "--------------\n",
    "\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up X and y, train-test split for \"full\"\n",
    "# X = full.drop('code', axis=1)\n",
    "# XExperiment = full.drop(['code', 'occ1data', 'fefF4data',\n",
    "#                          'fefF8data', 'pfcAF4data', 'motFC6data', 'parP8data',\n",
    "#                          'occ1SGFdata', 'fefF4SGFdata','fefF8SGFdata', 'pfcAF4SGFdata', #TODO change to pfc\n",
    "#                          'motFC6SGFdata', 'parP8SGFdata'], axis=1) \n",
    "# y = full['code']\n",
    "# print(X.columns, X.iloc[0], y.iloc[0], X.shape, y.shape)\n",
    "# XTrain, XTest, yTrain, yTest = train_test_split(XExperiment, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up X and y\n",
    "X = df.drop('code', axis=1)\n",
    "y = df['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O1     [4203.076923, 4193.333333, 4194.871794, 4207.1...\n",
      "O2     [4229.743589, 4216.923076, 4207.179487, 4215.3...\n",
      "F3     [4538.461538, 4528.717948, 4524.615384, 4526.1...\n",
      "F4     [4682.051282, 4667.179487, 4662.051282, 4669.2...\n",
      "F7     [4489.230769, 4475.384615, 4474.358974, 4486.6...\n",
      "F8     [3989.23077, 3983.589744, 3987.692308, 3991.79...\n",
      "T7     [4497.948717, 4498.461538, 4494.871794, 4497.9...\n",
      "T8     [4506.666666, 4501.025641, 4496.923076, 4496.9...\n",
      "AF3    [4395.384615, 4382.564102, 4377.435897, 4387.1...\n",
      "AF4    [4078.461538, 4062.564102, 4055.897435, 4065.6...\n",
      "FC5    [4207.692307, 4205.641025, 4200.51282, 4194.35...\n",
      "FC6    [4227.692307, 4215.384615, 4210.76923, 4221.02...\n",
      "P7     [4203.076923, 4192.820512, 4194.871794, 4204.6...\n",
      "P8     [4245.641025, 4236.410256, 4218.461538, 4220.0...\n",
      "Name: 0, dtype: object <class 'numpy.int64'> (65034, 14) (65034,)\n"
     ]
    }
   ],
   "source": [
    "#Since the csv is formatted wrong, something with read_csv is encoding the lists as strings, run this to turn them back to lists\n",
    "X = X.applymap(eval) # takes a while\n",
    "print(X.iloc[0], type(y.iloc[0]), X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XO1 = pd.DataFrame(X['O1'].tolist())\n",
    "# XO2 = pd.DataFrame(X['O2'].tolist())\n",
    "# XF3 = pd.DataFrame(X['F3'].tolist())\n",
    "# XF4 = pd.DataFrame(X['F4'].tolist())\n",
    "# XF7 = pd.DataFrame(X['F7'].tolist())\n",
    "# XF8 = pd.DataFrame(X['F8'].tolist())\n",
    "# XT7 = pd.DataFrame(X['T7'].tolist())\n",
    "# XT8 = pd.DataFrame(X['T8'].tolist())\n",
    "# XAF3 = pd.DataFrame(X['AF3'].tolist())\n",
    "# XAF4 = pd.DataFrame(X['AF4'].tolist())\n",
    "# XFC5 = pd.DataFrame(X['FC5'].tolist())\n",
    "# XFC6 = pd.DataFrame(X['FC6'].tolist())\n",
    "# XP7 = pd.DataFrame(X['P7'].tolist())\n",
    "# XP8 = pd.DataFrame(X['P8'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The columns are only labeled numerically, so there are 14 copies of the same set of names.\n",
    "# I don't think this would be a problem, but if it is, a unique list of col names can\n",
    "# be given when making each dataframe, at the X<channel name> = pd.DataFrame ... tolist(), cols =  \n",
    "# XWide = pd.concat([XO1, XO2, XF3, XF4, XF7, XF8, XT7, XT8, XAF3,\n",
    "#                    XAF4, XFC5, XFC6, XP7, XP8], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del X\n",
    "# del XO1\n",
    "# del XO2\n",
    "# del XF3\n",
    "# del XF4\n",
    "# del XF7\n",
    "# del XF8\n",
    "# del XT7\n",
    "# del XT8\n",
    "# del XAF3\n",
    "# del XAF4\n",
    "# del XFC5\n",
    "# del XFC6\n",
    "# del XP7\n",
    "# del XP8\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XWide.to_csv('../data/XWide.csv', sep=',')\n",
    "# y.to_csv('../data/y.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pre-made X set, wide version\n",
    "X = pd.read_csv('../data/XWide.csv').drop('Unnamed: 0', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('../data/y.csv').drop('Unnamed: 0', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split for \"df\"\n",
    "# XTrain, XTest, yTrain, yTest = train_test_split(X, y)\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246.13</th>\n",
       "      <th>247.13</th>\n",
       "      <th>248.13</th>\n",
       "      <th>249.13</th>\n",
       "      <th>250.13</th>\n",
       "      <th>251.13</th>\n",
       "      <th>252.13</th>\n",
       "      <th>253.13</th>\n",
       "      <th>254.13</th>\n",
       "      <th>255.13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8533</th>\n",
       "      <td>4435.897435</td>\n",
       "      <td>4436.923076</td>\n",
       "      <td>4440.000000</td>\n",
       "      <td>4439.487179</td>\n",
       "      <td>4438.974358</td>\n",
       "      <td>4441.025641</td>\n",
       "      <td>4440.512820</td>\n",
       "      <td>4438.974358</td>\n",
       "      <td>4443.076923</td>\n",
       "      <td>4447.692307</td>\n",
       "      <td>...</td>\n",
       "      <td>4195.384615</td>\n",
       "      <td>4198.461538</td>\n",
       "      <td>4194.871794</td>\n",
       "      <td>4194.358974</td>\n",
       "      <td>4202.564102</td>\n",
       "      <td>4214.358974</td>\n",
       "      <td>4215.384615</td>\n",
       "      <td>4207.179487</td>\n",
       "      <td>4203.076923</td>\n",
       "      <td>4202.564102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10641</th>\n",
       "      <td>4424.615384</td>\n",
       "      <td>4423.589743</td>\n",
       "      <td>4413.333333</td>\n",
       "      <td>4413.333333</td>\n",
       "      <td>4419.487179</td>\n",
       "      <td>4422.564102</td>\n",
       "      <td>4421.538461</td>\n",
       "      <td>4415.897435</td>\n",
       "      <td>4410.769230</td>\n",
       "      <td>4409.230769</td>\n",
       "      <td>...</td>\n",
       "      <td>4152.307692</td>\n",
       "      <td>4153.846153</td>\n",
       "      <td>4156.923076</td>\n",
       "      <td>4161.538461</td>\n",
       "      <td>4162.051282</td>\n",
       "      <td>4158.461538</td>\n",
       "      <td>4163.589743</td>\n",
       "      <td>4169.743589</td>\n",
       "      <td>4164.615384</td>\n",
       "      <td>4162.564102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>4440.000000</td>\n",
       "      <td>4441.025641</td>\n",
       "      <td>4443.589743</td>\n",
       "      <td>4437.948717</td>\n",
       "      <td>4437.948717</td>\n",
       "      <td>4443.076923</td>\n",
       "      <td>4436.410256</td>\n",
       "      <td>4425.128205</td>\n",
       "      <td>4429.230769</td>\n",
       "      <td>4437.435897</td>\n",
       "      <td>...</td>\n",
       "      <td>4257.948717</td>\n",
       "      <td>4250.769230</td>\n",
       "      <td>4240.512820</td>\n",
       "      <td>4238.974358</td>\n",
       "      <td>4241.538461</td>\n",
       "      <td>4246.666666</td>\n",
       "      <td>4251.794871</td>\n",
       "      <td>4248.717948</td>\n",
       "      <td>4245.128205</td>\n",
       "      <td>4249.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42297</th>\n",
       "      <td>4443.076923</td>\n",
       "      <td>4444.615384</td>\n",
       "      <td>4455.897435</td>\n",
       "      <td>4461.025641</td>\n",
       "      <td>4453.333333</td>\n",
       "      <td>4446.666666</td>\n",
       "      <td>4446.153846</td>\n",
       "      <td>4444.615384</td>\n",
       "      <td>4444.615384</td>\n",
       "      <td>4444.615384</td>\n",
       "      <td>...</td>\n",
       "      <td>4235.384615</td>\n",
       "      <td>4227.692307</td>\n",
       "      <td>4218.461538</td>\n",
       "      <td>4218.974358</td>\n",
       "      <td>4225.128205</td>\n",
       "      <td>4225.128205</td>\n",
       "      <td>4223.076923</td>\n",
       "      <td>4225.128205</td>\n",
       "      <td>4227.692307</td>\n",
       "      <td>4227.692307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37216</th>\n",
       "      <td>4410.256410</td>\n",
       "      <td>4410.769230</td>\n",
       "      <td>4416.410256</td>\n",
       "      <td>4425.128205</td>\n",
       "      <td>4426.153846</td>\n",
       "      <td>4414.358974</td>\n",
       "      <td>4405.641025</td>\n",
       "      <td>4407.179487</td>\n",
       "      <td>4412.307692</td>\n",
       "      <td>4420.512820</td>\n",
       "      <td>...</td>\n",
       "      <td>4224.615384</td>\n",
       "      <td>4222.051282</td>\n",
       "      <td>4210.769230</td>\n",
       "      <td>4202.564102</td>\n",
       "      <td>4207.179487</td>\n",
       "      <td>4215.384615</td>\n",
       "      <td>4218.461538</td>\n",
       "      <td>4210.256410</td>\n",
       "      <td>4198.974358</td>\n",
       "      <td>4204.102564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10808</th>\n",
       "      <td>4610.769230</td>\n",
       "      <td>4620.000000</td>\n",
       "      <td>4624.102564</td>\n",
       "      <td>4626.153846</td>\n",
       "      <td>4628.205128</td>\n",
       "      <td>4633.333333</td>\n",
       "      <td>4640.512820</td>\n",
       "      <td>4645.641025</td>\n",
       "      <td>4652.307692</td>\n",
       "      <td>4662.564102</td>\n",
       "      <td>...</td>\n",
       "      <td>4020.512820</td>\n",
       "      <td>4028.717948</td>\n",
       "      <td>4031.282051</td>\n",
       "      <td>4015.384615</td>\n",
       "      <td>3998.461539</td>\n",
       "      <td>4007.692307</td>\n",
       "      <td>4028.717948</td>\n",
       "      <td>4034.358974</td>\n",
       "      <td>4028.205128</td>\n",
       "      <td>4023.589743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>4407.179487</td>\n",
       "      <td>4409.743589</td>\n",
       "      <td>4410.769230</td>\n",
       "      <td>4411.794871</td>\n",
       "      <td>4414.358974</td>\n",
       "      <td>4414.871794</td>\n",
       "      <td>4413.846153</td>\n",
       "      <td>4415.384615</td>\n",
       "      <td>4414.358974</td>\n",
       "      <td>4411.282051</td>\n",
       "      <td>...</td>\n",
       "      <td>4242.564102</td>\n",
       "      <td>4234.358974</td>\n",
       "      <td>4230.769230</td>\n",
       "      <td>4231.794871</td>\n",
       "      <td>4227.179487</td>\n",
       "      <td>4229.230769</td>\n",
       "      <td>4246.153846</td>\n",
       "      <td>4244.102564</td>\n",
       "      <td>4223.076923</td>\n",
       "      <td>4218.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44105</th>\n",
       "      <td>4434.358974</td>\n",
       "      <td>4428.205128</td>\n",
       "      <td>4431.282051</td>\n",
       "      <td>4434.871794</td>\n",
       "      <td>4433.333333</td>\n",
       "      <td>4430.256410</td>\n",
       "      <td>4427.692307</td>\n",
       "      <td>4428.205128</td>\n",
       "      <td>4428.205128</td>\n",
       "      <td>4427.692307</td>\n",
       "      <td>...</td>\n",
       "      <td>4230.769230</td>\n",
       "      <td>4233.333333</td>\n",
       "      <td>4237.435897</td>\n",
       "      <td>4245.128205</td>\n",
       "      <td>4244.102564</td>\n",
       "      <td>4235.897435</td>\n",
       "      <td>4233.846153</td>\n",
       "      <td>4237.435897</td>\n",
       "      <td>4232.820512</td>\n",
       "      <td>4221.538461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36503</th>\n",
       "      <td>4406.666666</td>\n",
       "      <td>4403.589743</td>\n",
       "      <td>4411.282051</td>\n",
       "      <td>4416.410256</td>\n",
       "      <td>4414.358974</td>\n",
       "      <td>4414.358974</td>\n",
       "      <td>4418.974358</td>\n",
       "      <td>4419.487179</td>\n",
       "      <td>4417.435897</td>\n",
       "      <td>4421.025641</td>\n",
       "      <td>...</td>\n",
       "      <td>4204.615384</td>\n",
       "      <td>4207.179487</td>\n",
       "      <td>4215.897435</td>\n",
       "      <td>4218.461538</td>\n",
       "      <td>4213.846153</td>\n",
       "      <td>4210.769230</td>\n",
       "      <td>4212.820512</td>\n",
       "      <td>4216.923076</td>\n",
       "      <td>4216.923076</td>\n",
       "      <td>4211.794871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59551</th>\n",
       "      <td>4435.897435</td>\n",
       "      <td>4442.051282</td>\n",
       "      <td>4442.564102</td>\n",
       "      <td>4440.000000</td>\n",
       "      <td>4441.538461</td>\n",
       "      <td>4442.051282</td>\n",
       "      <td>4441.538461</td>\n",
       "      <td>4447.692307</td>\n",
       "      <td>4454.358974</td>\n",
       "      <td>4457.435897</td>\n",
       "      <td>...</td>\n",
       "      <td>4069.743589</td>\n",
       "      <td>4065.128205</td>\n",
       "      <td>4078.461538</td>\n",
       "      <td>4090.256410</td>\n",
       "      <td>4089.230769</td>\n",
       "      <td>4087.692307</td>\n",
       "      <td>4084.615384</td>\n",
       "      <td>4076.410256</td>\n",
       "      <td>4073.333333</td>\n",
       "      <td>4073.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48775 rows Ã— 3584 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2            3            4  \\\n",
       "8533   4435.897435  4436.923076  4440.000000  4439.487179  4438.974358   \n",
       "10641  4424.615384  4423.589743  4413.333333  4413.333333  4419.487179   \n",
       "1189   4440.000000  4441.025641  4443.589743  4437.948717  4437.948717   \n",
       "42297  4443.076923  4444.615384  4455.897435  4461.025641  4453.333333   \n",
       "37216  4410.256410  4410.769230  4416.410256  4425.128205  4426.153846   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "10808  4610.769230  4620.000000  4624.102564  4626.153846  4628.205128   \n",
       "1804   4407.179487  4409.743589  4410.769230  4411.794871  4414.358974   \n",
       "44105  4434.358974  4428.205128  4431.282051  4434.871794  4433.333333   \n",
       "36503  4406.666666  4403.589743  4411.282051  4416.410256  4414.358974   \n",
       "59551  4435.897435  4442.051282  4442.564102  4440.000000  4441.538461   \n",
       "\n",
       "                 5            6            7            8            9  ...  \\\n",
       "8533   4441.025641  4440.512820  4438.974358  4443.076923  4447.692307  ...   \n",
       "10641  4422.564102  4421.538461  4415.897435  4410.769230  4409.230769  ...   \n",
       "1189   4443.076923  4436.410256  4425.128205  4429.230769  4437.435897  ...   \n",
       "42297  4446.666666  4446.153846  4444.615384  4444.615384  4444.615384  ...   \n",
       "37216  4414.358974  4405.641025  4407.179487  4412.307692  4420.512820  ...   \n",
       "...            ...          ...          ...          ...          ...  ...   \n",
       "10808  4633.333333  4640.512820  4645.641025  4652.307692  4662.564102  ...   \n",
       "1804   4414.871794  4413.846153  4415.384615  4414.358974  4411.282051  ...   \n",
       "44105  4430.256410  4427.692307  4428.205128  4428.205128  4427.692307  ...   \n",
       "36503  4414.358974  4418.974358  4419.487179  4417.435897  4421.025641  ...   \n",
       "59551  4442.051282  4441.538461  4447.692307  4454.358974  4457.435897  ...   \n",
       "\n",
       "            246.13       247.13       248.13       249.13       250.13  \\\n",
       "8533   4195.384615  4198.461538  4194.871794  4194.358974  4202.564102   \n",
       "10641  4152.307692  4153.846153  4156.923076  4161.538461  4162.051282   \n",
       "1189   4257.948717  4250.769230  4240.512820  4238.974358  4241.538461   \n",
       "42297  4235.384615  4227.692307  4218.461538  4218.974358  4225.128205   \n",
       "37216  4224.615384  4222.051282  4210.769230  4202.564102  4207.179487   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "10808  4020.512820  4028.717948  4031.282051  4015.384615  3998.461539   \n",
       "1804   4242.564102  4234.358974  4230.769230  4231.794871  4227.179487   \n",
       "44105  4230.769230  4233.333333  4237.435897  4245.128205  4244.102564   \n",
       "36503  4204.615384  4207.179487  4215.897435  4218.461538  4213.846153   \n",
       "59551  4069.743589  4065.128205  4078.461538  4090.256410  4089.230769   \n",
       "\n",
       "            251.13       252.13       253.13       254.13       255.13  \n",
       "8533   4214.358974  4215.384615  4207.179487  4203.076923  4202.564102  \n",
       "10641  4158.461538  4163.589743  4169.743589  4164.615384  4162.564102  \n",
       "1189   4246.666666  4251.794871  4248.717948  4245.128205  4249.230769  \n",
       "42297  4225.128205  4223.076923  4225.128205  4227.692307  4227.692307  \n",
       "37216  4215.384615  4218.461538  4210.256410  4198.974358  4204.102564  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "10808  4007.692307  4028.717948  4034.358974  4028.205128  4023.589743  \n",
       "1804   4229.230769  4246.153846  4244.102564  4223.076923  4218.461538  \n",
       "44105  4235.897435  4233.846153  4237.435897  4232.820512  4221.538461  \n",
       "36503  4210.769230  4212.820512  4216.923076  4216.923076  4211.794871  \n",
       "59551  4087.692307  4084.615384  4076.410256  4073.333333  4073.333333  \n",
       "\n",
       "[48775 rows x 3584 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ________________________\r\n",
      "< moooooo... braaains... >\r\n",
      " ------------------------\r\n",
      "        \\   ^__^\r\n",
      "         \\  (oo)\\_______\r\n",
      "            (__)\\       )\\/\\\r\n",
      "                ||----w |\r\n",
      "                ||     ||\r\n"
     ]
    }
   ],
   "source": [
    "!cowsay moooooo... braaains..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48775, 256), (16259, 256), (48775,), (16259,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrain.shape, XTest.shape, yTrain.shape, yTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XTrainNP = XTrain.applymap(np.array)\n",
    "# # type(XTrainNP['T7'][0])\n",
    "# XTrainNP = XTrainNP.to_numpy()\n",
    "# XTrainNP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15Gi       6.9Gi       2.9Gi       238Mi       5.7Gi       8.1Gi\r\n",
      "Swap:          976Mi       374Mi       602Mi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XTrainNP = tf.reshape(np.array(XTrainNP), (48775, 256, 1, 14))\n",
    "# XTrainNP = np.reshape(XTrainNP, newshape=(256, 1, 14))\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "# # to fix error, try this: https://www.youtube.com/watch?v=k_VAKyzggJI\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# lb = LabelEncoder()\n",
    "# yTrainOHE = lb.fit_transform(yTrain)\n",
    "yTrainOHE = ohe.fit_transform(yTrain.to_numpy().reshape(-1,1))\n",
    "\n",
    "# ohe0 = OneHotEncoder(sparse=False)\n",
    "# yTestOHE = ohe0.fit(yTrain.to_numpy().reshape(-1,1)) # Daniel said to do this but I can't remember why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48775, 11)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTrainOHE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=64, kernel_size=1, activation='relu', input_shape=(48775, 256, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Conv2D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Conv2D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "# model.add(LSTM(7, input_shape=(48775, 256, 1), activation='tanh', \n",
    "#                recurrent_activation='sigmoid'))\n",
    "# model.add(LSTM(7, input_shape=(48775, 256, 1), activation='tanh', \n",
    "#                recurrent_activation='sigmoid')\n",
    "# model.add(LSTM(7, input_shape=(48775, 256, 1), activation='tanh', \n",
    "#                recurrent_activation='sigmoid')\n",
    "## model.add(Dense(32, activation='relu'))\n",
    "## model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(11, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_7 (Conv1D)           (None, 256, 64)           128       \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 256, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 256, 64)           4160      \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 256, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 256, 64)           4160      \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPooling  (None, 256, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                524320    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 11)                363       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 534,187\n",
      "Trainable params: 534,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "results = model.fit(XTrain, yTrainOHE, batch_size=20,epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best accuracy for the unprocessed signal with all channels was about .1020\n",
    "\n",
    "In 6/14 channels, adding SGF data was about the same\n",
    "\n",
    "In 6/14 channels, adding SGF and dropping raw data resulted in really low accuracy and NaN loss\n",
    "\n",
    "In 6/14 channels, ICA only had a accuracy of about .1000\n",
    "\n",
    "6/14 channels, unproccessed+SGF+ICA, 22,059 params: NaN loss again, what's wrong with the data? woe be the futility of mine ways\n",
    "\n",
    "6/14 channels, SGF only, 9,771 params, accuracy still around .1000\n",
    "\n",
    "6/14 channels, raw, 9,771 params, accuracy about .1000, so the .0020 increase was only from the extra channels. Processing doesn't seem to change the accuracy at all.\n",
    "\n",
    "----------\n",
    "Using vector data rows (\"wide\"), I get this error:\n",
    "```\n",
    "UnimplementedError                        Traceback (most recent call last)\n",
    "<ipython-input-59-f6344dc87bbc> in <module>\n",
    "      1 model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "----> 2 results = model.fit(XTrain, yTrainOHE, batch_size=20,epochs=30, verbose=1)\n",
    "\n",
    "...\n",
    "\n",
    "Cast string to float is not supported\n",
    "\t [[{{node sequential_4/Cast}}]] [Op:__inference_train_function_2630]\n",
    "```\n",
    "\n",
    "but the prep function seems to work fine: column is series, col[0] is list, and col[0][0] is float\n",
    "```\n",
    "(learn-env) :~/flatiron/cap/capstoneProject/workingNotebooks$ python dataPrepV2.py \n",
    "<class 'pandas.core.series.Series'> <class 'list'> <class 'float'>\n",
    "```\n",
    "\n",
    "-------------\n",
    "\n",
    "Using one channel with each data row expanded as a list then made into a new df, the loss gave a NaN error and the accuracy hovered around 0.002 (Conv1D)\n",
    "\n",
    "Attempting a Conv2D version with input shape 48775, 256, 1 gives the following memory overload error:\n",
    "\n",
    "```\n",
    "ResourceExhaustedError: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[799129600,32] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:StatelessRandomUniformV2]\n",
    "```\n",
    "\n",
    "I believe the temporal + amplitude data in the raw readings would be maintained using Conv2D layers, but these data trigger an error in TF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(XTrain.fillna(XTrain.mean()), yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11212251675994833"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(XTest.fillna(XTest.mean()), yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick support vector classifier ended up getting the same accuracy, 0.10234331754720463\n",
    "\n",
    "Another with the one channel \"wide\" format gives an accuracy of 0.10910880127929147, which is interesting because it's better than the baseline CNN, and it's 1 out of 14 channels unfiltered. Maybe a not-so-bad model will be a possibility with all channels, especially if they're filtered.\n",
    "\n",
    "On the full, 14 channel dataset, I got my highest score yet, 0.11212251675994833! now we're getting somewhere lol. Tomorrow I will try the filtered version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Echo State Network Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reservoirpy as rpy\n",
    "from reservoirpy.nodes import Reservoir, Ridge, FORCE, ESN\n",
    "\n",
    "readout = Ridge(ridge=1e-7)\n",
    "train_states = Reservoir.run(XTrain, reset=True)\n",
    "# train_statesy = reservoir.run(yTrain.to_numpy().reshape(-1, 1), reset=True)\n",
    "readout = readout.fit(train_states, yTrain.to_numpy().reshape(-1, 1), warmup=10)\n",
    "test_states = reservoir.run(XTest.to_numpy().reshape(-1, 1))\n",
    "yPred = readout.run(test_states)\n",
    "\n",
    "reservoir = Reservoir(100, lr=0.5, sr=0.9)\n",
    "ridge = Ridge(ridge=1e-7)\n",
    "\n",
    "esn_model = reservoir >> ridge\n",
    "\n",
    "esn_model = esn_model.fit(XTrain.to_numpy().reshape(-1, 1), yTrain.to_numpy().reshape(-1, 1).to_numpy().reshape(-1, 1), warmup=10)\n",
    "print(reservoir.is_initialized, readout.is_initialized, readout.fitted)\n",
    "\n",
    "yPred = esn_model.run(XTest.to_numpy().reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it was nice to learn how to set one of these up, I realized after a lot of tuning the reservoirpy library doesn't seem to have good interop with pandas. Might have interesting results with 1D timestamp and 1D value arrays, but not a multidimensional dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "readout1 = Ridge(ridge=1e-7)\n",
    "train_states1 = reservoir.run(full['pfcAF4SGFdata'].to_numpy().reshape(-1, 1), reset=True)\n",
    "# train_statesy = reservoir.run(yTrain.to_numpy().reshape(-1, 1), reset=True)\n",
    "readout1 = readout1.fit(train_states1, full['code'].to_numpy().reshape(-1, 1), warmup=10)\n",
    "test_states1 = reservoir.run((full['pfcAF4SGFdata'].to_numpy().reshape(-1, 1)))\n",
    "# yPred1 = readout.run(test_states1)\n",
    "\n",
    "reservoir1 = Reservoir(100, lr=0.5, sr=0.9)\n",
    "ridge1 = Ridge(ridge=1e-7)\n",
    "\n",
    "esn_model1 = reservoir >> ridge\n",
    "\n",
    "esn_model1 = esn_model1.fit(full['pfcAF4SGFdata'].to_numpy().reshape(-1, 1), \n",
    "                             full['code'].to_numpy().reshape(-1, 1) ,warmup=10)\n",
    "print(reservoir.is_initialized, readout.is_initialized, readout.fitted)\n",
    "\n",
    "yPred1 = esn_model.run((full['pfcAF4SGFdata'].to_numpy().reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4.494217014863551}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(yPred1.flatten()), (mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use exploded data of one event from original CSV, predict brain activity with ESN:\n",
    "# forecast, generate artificial brainwaves of looking at a digit\n",
    "\n",
    "# TODO explore possibility of using matplotlib plots of signals as training data\n",
    "# like in https://arxiv.org/abs/1511.06448\n",
    "\n",
    "# in the future, try iterating ESNpredict -> ESNtrain -> ESNpredict -> ESNtrain -> ...\n",
    "# to see how predictions change over time, maybe there would be a \"core\" shape that can be\n",
    "# like a waveform decomposition of the number, \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
