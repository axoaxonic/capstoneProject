{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE MODEL\n",
    "-----------------\n",
    "-----------------\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, GlobalMaxPooling2D, LSTM, MaxPooling2D, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15Gi       7.5Gi       3.1Gi       247Mi       4.9Gi       7.5Gi\r\n",
      "Swap:          976Mi       0.0Ki       976Mi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This df will be a row-concatenated version of the same channels\n",
    "# baseline = pd.read_csv('../../baseline.csv', delimiter=',')\n",
    "# baseline.drop('Unnamed: 0', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/dfTensor2.csv', delimiter=';', encoding='latin-1')\n",
    "df.drop('Unnamed: 0', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[3989.23077, 3983.589744, 3987.692308, 3991.794872, 3992.307693, 3990.256411, 3993.333334, 4000.0, 3994.871795, 3985.128206, 3995.897436, 4001.538461, 3986.153847, 3982.051283, 3990.256411, 3994.358975, 3981.538462, 3961.025642, 3968.205129, 3989.23077, 3980.512821, 3973.333334, 3997.948718, 4009.230769, 3998.974359, 3998.461539, 3999.48718, 3997.435898, 3999.48718, 4001.025641, 3994.871795, 3987.179488, 3987.692308, 3992.307693, 3987.692308, 3980.0, 3984.102565, 3994.358975, 3991.794872, 3982.564103, 3980.0, 3978.461539, 3978.461539, 3989.23077, 3998.461539, 3998.461539, 3996.410257, 3978.974359, 3966.153847, 3978.974359, 3987.692308, 3985.128206, 3993.846154, 3995.384616, 3974.871795, 3957.948718, 3965.128206, 3990.769231, 4007.179487, 3998.974359, 3981.025642, 3970.256411, 3976.923077, 3993.333334, 3992.820513, 3978.461539, 3976.923077, 3975.897436, 3973.846154, 3984.615385, 3986.153847, 3978.974359, 3980.0, 3982.051283, 3981.538462, 3983.589744, 3978.974359, 3971.282052, 3972.820513, 3971.794872, 3969.23077, 3976.923077, 3975.897436, 3967.692308, 3972.820513, 3984.102565, 3983.589744, 3974.871795, 3967.692308, 3966.153847, 3974.358975, 3981.025642, 3971.282052, 3963.076924, 3973.846154, 3986.153847, 3983.076924, 3981.025642, 3986.666667, 3984.615385, 3977.435898, 3975.384616, 3974.358975, 3973.333334, 3978.461539, 3983.589744, 3984.615385, 3990.256411, 3988.205129, 3974.358975, 3983.589744, 4000.51282, 3986.153847, 3969.23077, 3979.48718, 3987.692308, 3980.512821, 3980.512821, 3994.358975, 4001.538461, 3988.717949, 3981.025642, 3992.820513, 3996.923077, 3988.717949, 3981.538462, 3973.846154, 3970.769231, 3982.051283, 3998.461539, 3999.48718, 3989.74359, 3984.615385, 3989.23077, 3988.205129, 3971.282052, 3969.74359, 3990.256411, 3990.769231, 3967.692308, 3963.076924, 3978.974359, 3987.692308, 3981.538462, 3974.871795, 3986.666667, 3994.871795, 3980.512821, 3975.384616, 3986.153847, 3984.615385, 3980.512821, 3985.641026, 3986.153847, 3980.512821, 3986.153847, 3991.794872, 3978.461539, 3970.256411, 3985.128206, 3989.23077, 3982.051283, 3987.692308, 3990.769231, 3982.564103, 3989.74359, 4000.0, 3985.641026, 3973.333334, 3989.74359, 4012.307692, 4016.923076, 4009.743589, 4008.717948, 4010.76923, 4005.641025, 3998.461539, 3997.435898, 3997.948718, 3994.871795, 3992.307693, 3991.282052, 3989.23077, 3990.256411, 3994.358975, 3997.948718, 4003.589743, 4008.717948, 4008.717948, 3998.974359, 3985.128206, 3985.128206, 3996.923077, 3999.48718, 3990.256411, 3983.589744, 3982.564103, 3981.025642, 3981.025642, 3988.717949, 3990.769231, 3986.666667, 3996.410257, 4000.51282, 3987.692308, 3982.564103, 3986.153847, 3990.769231, 4001.538461, 4006.153846, 3994.358975, 3982.051283, 3986.153847, 4000.0, 3999.48718, 3986.666667, 3985.128206, 3991.794872, 3996.923077, 4004.102564, 3997.948718, 3985.128206, 3994.871795, 4007.179487, 3995.897436, 3991.282052, 4007.692307, 4013.846153, 4003.076923, 3994.871795, 3997.948718, 4008.717948, 4013.846153, 4005.641025, 3995.384616, 3993.846154, 3999.48718, 4006.666666, 4008.717948, 4004.615384, 3999.48718, 3993.333334, 3993.333334, 4000.51282, 4001.538461, 3997.948718, 3999.48718, 4001.025641, 3996.410257, 3996.923077, 3997.435898, 3994.358975, 3996.410257, 4001.025641, 4005.641025, 4014.871794]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['F8'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['O1'] = df['O1'].apply(eval)\n",
    "# df['O1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataframes for each channel, pick and choose at will\n",
    "\n",
    "# occ0 = pd.read_csv('../data/occ0Exp.csv', delimiter=',')\n",
    "# occ1 = pd.read_csv('../data/occ1Exp.csv', delimiter=',')\n",
    "# fefF3 = pd.read_csv('../data/fefF3Exp.csv', delimiter=',')\n",
    "# fefF4 = pd.read_csv('../data/fefF4Exp.csv', delimiter=',')\n",
    "# fefF7 = pd.read_csv('../data/fefF7Exp.csv', delimiter=',')\n",
    "# fefF8 = pd.read_csv('../data/fefF8Exp.csv', delimiter=',')\n",
    "# temT7 = pd.read_csv('../data/temT7Exp.csv', delimiter=',')\n",
    "# temT8 = pd.read_csv('../data/temT8Exp.csv', delimiter=',')\n",
    "# pfcAF3 = pd.read_csv('../data/pfcAF3Exp.csv', delimiter=',')\n",
    "# pfcAF4 = pd.read_csv('../data/pfcAF4Exp.csv', delimiter=',')\n",
    "# motFC5 = pd.read_csv('../data/motFC5Exp.csv', delimiter=',')\n",
    "# motFC6 = pd.read_csv('../data/motFC6Exp.csv', delimiter=',')\n",
    "# parP7 = pd.read_csv('../data/parP7Exp.csv', delimiter=',')\n",
    "# parP8 = pd.read_csv('../data/parP8Exp.csv', delimiter=',')\n",
    "\n",
    "# loading same datasets expanded with filter processing\n",
    "\n",
    "# occ0 = pd.read_csv('../data/occ0Proc.csv', delimiter=',')\n",
    "# occ1 = pd.read_csv('../data/occ1Proc.csv', delimiter=',') \n",
    "# fefF3 = pd.read_csv('../data/fefF3Proc.csv', delimiter=',')\n",
    "# fefF4 = pd.read_csv('../data/fefF4Proc.csv', delimiter=',')\n",
    "# fefF7 = pd.read_csv('../data/fefF7Proc.csv', delimiter=',')\n",
    "# fefF8 = pd.read_csv('../data/fefF8Proc.csv', delimiter=',') \n",
    "# temT7 = pd.read_csv('../data/temT7Proc.csv', delimiter=',')\n",
    "# temT8 = pd.read_csv('../data/temT8Proc.csv', delimiter=',')\n",
    "# pfcAF3 = pd.read_csv('../data/pfcAF3Proc.csv', delimiter=',')\n",
    "# pfcAF4 = pd.read_csv('../data/pfcAF4Proc.csv', delimiter=',')\n",
    "# motFC5 = pd.read_csv('../data/motFC5Proc.csv', delimiter=',')\n",
    "# motFC6 = pd.read_csv('../data/motFC6Proc.csv', delimiter=',')\n",
    "# parP7 = pd.read_csv('../data/parP7Proc.csv', delimiter=',')\n",
    "# parP8 = pd.read_csv('../data/parP8Proc.csv', delimiter=',')\n",
    "\n",
    "# occ0 = pd.read_csv('../data/occ0Wide.csv', delimiter=',')\n",
    "# occ1 = pd.read_csv('../data/occ1Wide.csv', delimiter=',') \n",
    "# fefF3 = pd.read_csv('../data/fefF3Wide.csv', delimiter=',')\n",
    "# fefF4 = pd.read_csv('../data/fefF4Wide.csv', delimiter=',')\n",
    "# fefF7 = pd.read_csv('../data/fefF7Wide.csv', delimiter=',')\n",
    "# fefF8 = pd.read_csv('../data/fefF8Wide.csv', delimiter=',') \n",
    "# temT7 = pd.read_csv('../data/temT7Wide.csv', delimiter=',')\n",
    "# temT8 = pd.read_csv('../data/temT8Wide.csv', delimiter=',')\n",
    "# pfcAF3 = pd.read_csv('../data/pfcAF3Wide.csv', delimiter=',')\n",
    "# pfcAF4 = pd.read_csv('../data/pfcAF4Wide.csv', delimiter=',')\n",
    "# motFC5 = pd.read_csv('../data/motFC5Wide.csv', delimiter=',')\n",
    "# motFC6 = pd.read_csv('../data/motFC6Wide.csv', delimiter=',')\n",
    "# parP7 = pd.read_csv('../data/parP7Wide.csv', delimiter=',')\n",
    "# parP8 = pd.read_csv('../data/parP8Wide.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del occ0\n",
    "# del occ1\n",
    "# del fefF3\n",
    "# del fefF4\n",
    "# del fefF7\n",
    "# del fefF8\n",
    "# del temT7\n",
    "# del temT8\n",
    "# del pfcAF3\n",
    "# del pfcAF4\n",
    "# del motFC5\n",
    "# del motFC6\n",
    "# del parP7\n",
    "# del parP8\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading only the dataframes from channels positively correlated with code, to save RAM\n",
    "# corrcols = [occ1, fefF4, fefF8, pfcAF4, motFC6, parP8] #removed motFC5 for being a diff shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations of averaged channels, if there is a need to pick and choose to save RAM:\n",
    "\n",
    "<sub>\n",
    "code          1.000000\n",
    "    \n",
    "&nbsp;\n",
    "    \n",
    "occ0Data     -0.000385\n",
    "\n",
    "&nbsp;\n",
    "    \n",
    "occ1Data      0.001626\n",
    "    \n",
    "&nbsp;\n",
    "\n",
    "fefF3Data    -0.005483\n",
    "\n",
    "&nbsp;\n",
    "    \n",
    "fefF4Data     0.008288\n",
    "\n",
    "&nbsp;\n",
    "    \n",
    "fefF7Data    -0.004799\n",
    "\n",
    "&nbsp;\n",
    "        \n",
    "fefF8Data     0.006907\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "temT7Data    -0.002657\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "temT8Data    -0.001037\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "pfcAF3Data   -0.007512\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "pfcAF4Data    0.007591\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "motFC5Data    0.003757\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "motFC6Data    0.001372\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "parP7Data    -0.003000\n",
    "\n",
    "&nbsp;    \n",
    "    \n",
    "parP8Data     0.002366\n",
    "</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15Gi       7.7Gi       946Mi       562Mi       6.9Gi       6.9Gi\r\n",
      "Swap:          976Mi        11Mi       965Mi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If individual columns are loaded, use this cell to process them and aggregate group them by event\n",
    "# This gets rid of the time dimension though and will make the model be highly inaccurate\n",
    "\n",
    "# dfs = [occ0, occ1, fefF3, fefF4, fefF7, fefF8, \n",
    "#        temT7, temT8, pfcAF3, pfcAF4, motFC5, motFC6, parP7, parP8]\n",
    "# for i in dfs: #dfs:\n",
    "# #     i.drop('Unnamed: 0.1', inplace=True, axis=1)\n",
    "#     i.drop(['Unnamed: 0', 'size'], inplace=True, axis=1)\n",
    "#     i['data'].astype(float, copy=False)\n",
    "#     print(i.columns)\n",
    "# #     i = i.groupby('event').mean()\n",
    "# #     print(i.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non loop version of above's last line\n",
    "# occ1, fefF4, fefF8, pfcAF4, motFC5, motFC6, parP8 for corrcols\n",
    "\n",
    "# occ0 = occ0.groupby('event').mean()\n",
    "# occ1 = occ1.groupby('event').mean()\n",
    "# fefF3 = fefF3.groupby('event').mean()\n",
    "# fefF4 = fefF4.groupby('event').mean()\n",
    "# fefF7 = fefF7.groupby('event').mean()\n",
    "# fefF8 = fefF8.groupby('event').mean()\n",
    "# temT7 = temT7.groupby('event').mean()\n",
    "# temT8 = temT8.groupby('event').mean()\n",
    "# pfcAF3 = pfcAF3.groupby('event').mean()\n",
    "# pfcAF4 = pfcAF4.groupby('event').mean()\n",
    "# motFC5 = motFC5.groupby('event').mean() #this one has less rows than the rest, why?\n",
    "# motFC6 = motFC6.groupby('event').mean()\n",
    "# parP7 = parP7.groupby('event').mean()\n",
    "# parP8 = parP8.groupby('event').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Check each channel dataframe for null vals \n",
    "# for i in corrcols:\n",
    "#     print(max(i.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Renaming data column to specify channel name in individual channel dataframes, so when they are concatenated, the full df doesn't have multiple cols with the same names\n",
    "## (use  occ1, fefF4, fefF8, pfcAF4, motFC6, parP8 for corrcols)\n",
    "\n",
    "# occ0.rename(columns={'data': 'occ0data'}, inplace=True)\n",
    "# occ1.rename(columns={'data':'occ1data', 'ICAdata':'occ1ICAdata', 'SGFdata':'occ1SGFdata'}, inplace=True)\n",
    "# fefF3.rename(columns={'data': 'fefF3data'}, inplace=True)\n",
    "# fefF4.rename(columns={'data':'fefF4data', 'ICAdata':'fefF4ICAdata', 'SGFdata':'fefF4SGFdata'}, inplace=True)\n",
    "# fefF7.rename(columns={'data': 'fefF7data'}, inplace=True)\n",
    "# fefF8.rename(columns={'data':'fefF8data', 'ICAdata':'fefF8ICAdata', 'SGFdata':'fefF8SGFdata'}, inplace=True)\n",
    "# temT7.rename(columns={'data': 'temT7data'}, inplace=True)\n",
    "# temT8.rename(columns={'data': 'temT8data'}, inplace=True)\n",
    "# pfcAF3.rename(columns={'data': 'pfcAF3data'}, inplace=True)\n",
    "# pfcAF4.rename(columns={'data':'pfcAF4data', 'ICAdata':'pfcAF4ICAdata', 'SGFdata':'pfcAF4SGFdata'}, inplace=True)\n",
    "# motFC5.rename(columns={'data': 'motFC5data'}, inplace=True)\n",
    "# motFC6.rename(columns={'data':'motFC6data', 'ICAdata':'motFC6ICAdata', 'SGFdata':'motFC6SGFdata'}, inplace=True)\n",
    "# parP7.rename(columns={'data': 'parP7data'}, inplace=True)\n",
    "# parP8.rename(columns={'data':'parP8data', 'ICAdata':'parP8ICAdata', 'SGFdata':'parP8SGFdata'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occ0.columns, occ1.columns, fefF7.columns, fefF8.columns\n",
    "# # dfs[0] #this wasn't changed with the above functions, it's only used for calling the original datasets as they were read in\n",
    "# dfsProcessed = [occ0, occ1, fefF3, fefF4, fefF7, fefF8, \n",
    "#        temT7, temT8, pfcAF3, pfcAF4, motFC5, motFC6, parP7, parP8]\n",
    "\n",
    "## Remaking this list included processed dataframes\n",
    "# corrcolsProcessed = [occ1, fefF4, fefF8, pfcAF4, motFC6, parP8] #new vars because the original list has the unprocessed dataframes in memory it seems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If using individual channel dataframes, use this cell to turn them into one df\n",
    "# full = pd.concat(corrcolsProcessed, axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full.columns.duplicated()\n",
    "# # Since the concat created duplicate code columns, using this trick to delete them:\n",
    "# #https://stackoverflow.com/questions/14984119/python-pandas-remove-duplicate-columns\n",
    "# #Although a different type of join might prevent the need for this\n",
    "# full = full.loc[:,~full.columns.duplicated()].copy()\n",
    "# #full = full.drop(['size', 'id' ], axis=1) # If any extraneous cols are leftover\n",
    "# full.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrMatFull = full.corr()\n",
    "#corrMatFull['code']\n",
    "# check = pd.read_csv('../data/occ1Exp.csv', delimiter=',')\n",
    "# print(check.corr()) # code correlates to data in this channel by 0.001562 \n",
    "# del check \n",
    "\n",
    "# check = pd.read_csv('../data/fefF7Exp.csv', delimiter=',')\n",
    "# print(check.corr()) # code correlates to data in this channel by -0.004390.\n",
    "# del check \n",
    "\n",
    "# check = pd.read_csv('../data/occ0Exp.csv', delimiter=',')\n",
    "# print(check.corr()) # code correlates to data in this channel by -0.000308 \n",
    "# del check \n",
    "\n",
    "# check = pd.read_csv('../data/fefF8Exp.csv', delimiter=',')\n",
    "# print(check.corr()) # code correlates to data in this channel by 0.006184\n",
    "# del check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# full.plot(y=['occ0Data', 'occ1Data', 'fefF7Data', 'fefF8Data'], alpha=.7 ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15Gi        13Gi       334Mi       227Mi       1.7Gi       1.4Gi\r\n",
      "Swap:          976Mi        45Mi       931Mi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Baseline Model: Only O1, O2, F7, F8; No Preprocessing\n",
    "## Baseline+ : Basline+Preprocessing\n",
    "\n",
    "--------------\n",
    "\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[4203.076923, 4193.333333, 4194.871794, 4207.179487, 4220.51282, 4222.564102, 4217.435897, 4212.307692, 4210.25641, 4214.358974, 4216.923076, 4210.76923, 4210.25641, 4215.384615, 4216.410256, 4214.358974, 4212.307692, 4211.282051, 4212.820512, 4215.897435, 4214.358974, 4213.846153, 4224.102564, 4231.282051, 4230.25641, 4229.743589, 4222.051282, 4214.358974, 4218.974358, 4222.564102, 4218.974358, 4223.589743, 4230.25641, 4230.76923, 4227.692307, 4224.102564, 4227.179487, 4232.307692, 4230.76923, 4229.743589, 4233.333333, 4233.333333, 4232.820512, 4236.410256, 4234.871794, 4232.307692, 4238.974358, 4249.230769, 4251.794871, 4249.230769, 4246.153846, 4244.615384, 4246.153846, 4246.153846, 4247.179487, 4257.948717, 4267.692307, 4268.205128, 4265.641025, 4266.153846, 4264.102564, 4261.538461, 4267.692307, 4271.282051, 4265.128205, 4267.692307, 4282.051282, 4285.128205, 4280.0, 4287.692307, 4296.410256, 4289.743589, 4279.487179, 4277.435897, 4279.487179, 4286.153846, 4295.384615, 4297.435897, 4295.897435, 4294.358974, 4285.128205, 4277.435897, 4283.589743, 4287.692307, 4289.743589, 4299.487179, 4304.102564, 4294.358974, 4290.76923, 4294.871794, 4291.794871, 4293.846153, 4303.589743, 4302.564102, 4304.102564, 4309.230769, 4303.589743, 4299.487179, 4307.692307, 4312.307692, 4309.743589, 4309.743589, 4309.230769, 4309.743589, 4311.282051, 4312.820512, 4314.871794, 4312.820512, 4310.76923, 4314.871794, 4321.025641, 4320.0, 4311.794871, 4306.666666, 4311.282051, 4319.487179, 4322.564102, 4321.025641, 4319.487179, 4319.487179, 4319.487179, 4321.538461, 4325.128205, 4325.128205, 4324.615384, 4326.666666, 4326.666666, 4328.205128, 4333.846153, 4337.435897, 4335.384615, 4330.25641, 4326.153846, 4327.179487, 4326.666666, 4322.564102, 4329.230769, 4335.384615, 4326.153846, 4324.615384, 4337.435897, 4345.128205, 4342.051282, 4337.435897, 4334.871794, 4334.871794, 4330.76923, 4323.589743, 4326.153846, 4333.846153, 4335.384615, 4337.435897, 4337.948717, 4334.358974, 4336.410256, 4342.051282, 4339.487179, 4335.384615, 4336.410256, 4337.948717, 4338.461538, 4342.564102, 4350.25641, 4350.76923, 4347.692307, 4346.666666, 4345.128205, 4349.743589, 4356.923076, 4355.384615, 4353.333333, 4358.974358, 4357.948717, 4355.384615, 4357.948717, 4356.923076, 4355.384615, 4356.410256, 4357.435897, 4356.923076, 4356.923076, 4358.461538, 4358.461538, 4358.974358, 4358.974358, 4357.948717, 4360.0, 4366.666666, 4367.179487, 4361.538461, 4362.564102, 4363.589743, 4364.615384, 4366.153846, 4363.589743, 4367.692307, 4376.410256, 4375.897435, 4369.230769, 4370.25641, 4375.384615, 4381.538461, 4387.692307, 4378.974358, 4366.153846, 4373.333333, 4383.076923, 4377.435897, 4373.333333, 4378.461538, 4374.871794, 4368.717948, 4369.230769, 4366.153846, 4364.102564, 4370.25641, 4374.358974, 4373.333333, 4372.820512, 4373.846153, 4372.307692, 4373.333333, 4379.487179, 4381.025641, 4377.948717, 4380.0, 4385.641025, 4384.615384, 4381.538461, 4384.615384, 4386.666666, 4382.564102, 4380.0, 4384.102564, 4387.692307, 4384.102564, 4376.923076, 4381.538461, 4394.871794, 4396.410256, 4388.717948, 4386.666666, 4385.641025, 4382.564102, 4383.589743, 4384.102564, 4385.128205, 4389.230769, 4386.666666, 4383.076923, 4385.128205, 4383.589743, 4381.538461, 4389.230769, 4394.358974, 4389.743589, 4385.128205]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up X and y, train-test split for \"full\"\n",
    "# X = full.drop('code', axis=1)\n",
    "# XExperiment = full.drop(['code', 'occ1data', 'fefF4data',\n",
    "#                          'fefF8data', 'pfcAF4data', 'motFC6data', 'parP8data',\n",
    "#                          'occ1SGFdata', 'fefF4SGFdata','fefF8SGFdata', 'pfcAF4SGFdata', #TODO change to pfc\n",
    "#                          'motFC6SGFdata', 'parP8SGFdata'], axis=1) \n",
    "# y = full['code']\n",
    "# print(X.columns, X.iloc[0], y.iloc[0], X.shape, y.shape)\n",
    "# XTrain, XTest, yTrain, yTest = train_test_split(XExperiment, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up X and y\n",
    "X = df.drop('code', axis=1)\n",
    "y = df['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O1     [4203.076923, 4193.333333, 4194.871794, 4207.1...\n",
      "O2     [4229.743589, 4216.923076, 4207.179487, 4215.3...\n",
      "F3     [4538.461538, 4528.717948, 4524.615384, 4526.1...\n",
      "F4     [4682.051282, 4667.179487, 4662.051282, 4669.2...\n",
      "F7     [4489.230769, 4475.384615, 4474.358974, 4486.6...\n",
      "F8     [3989.23077, 3983.589744, 3987.692308, 3991.79...\n",
      "T7     [4497.948717, 4498.461538, 4494.871794, 4497.9...\n",
      "T8     [4506.666666, 4501.025641, 4496.923076, 4496.9...\n",
      "AF3    [4395.384615, 4382.564102, 4377.435897, 4387.1...\n",
      "AF4    [4078.461538, 4062.564102, 4055.897435, 4065.6...\n",
      "FC5    [4207.692307, 4205.641025, 4200.51282, 4194.35...\n",
      "FC6    [4227.692307, 4215.384615, 4210.76923, 4221.02...\n",
      "P7     [4203.076923, 4192.820512, 4194.871794, 4204.6...\n",
      "P8     [4245.641025, 4236.410256, 4218.461538, 4220.0...\n",
      "Name: 0, dtype: object <class 'numpy.int64'> (65034, 14) (65034,)\n"
     ]
    }
   ],
   "source": [
    "#Since the csv is formatted wrong, something with read_csv is encoding the lists as strings, run this to turn them back to lists\n",
    "X = X.applymap(eval) # takes a while\n",
    "print(X.iloc[0], type(y.iloc[0]), X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split for \"df\"\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X['T7'].iloc[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ________________________\r\n",
      "< moooooo... braaains... >\r\n",
      " ------------------------\r\n",
      "        \\   ^__^\r\n",
      "         \\  (oo)\\_______\r\n",
      "            (__)\\       )\\/\\\r\n",
      "                ||----w |\r\n",
      "                ||     ||\r\n"
     ]
    }
   ],
   "source": [
    "!cowsay moooooo... braaains..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48775, 14), (16259, 14), (48775,), (16259,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrain.shape, XTest.shape, yTrain.shape, yTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2084db35d4ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48775, 14)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrainNP = XTrain.applymap(np.array)\n",
    "# type(XTrainNP['T7'][0])\n",
    "XTrainNP = XTrainNP.to_numpy()\n",
    "XTrainNP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15Gi        10Gi       555Mi       286Mi       4.5Gi       4.4Gi\r\n",
      "Swap:          976Mi        25Mi       951Mi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(682850,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrainNP.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "682850"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "48775*14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 682850 into shape (256,1,14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a3114c559325>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# XTrainNP = tf.reshape(np.array(XTrainNP), (48775, 256, 1, 14))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mXTrainNP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXTrainNP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# ohe = OneHotEncoder(sparse=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# # to fix error, try this: https://www.youtube.com/watch?v=k_VAKyzggJI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# from sklearn.preprocessing import LabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    296\u001b[0m            [5, 6]])\n\u001b[1;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 682850 into shape (256,1,14)"
     ]
    }
   ],
   "source": [
    "# XTrainNP = tf.reshape(np.array(XTrainNP), (48775, 256, 1, 14))\n",
    "XTrainNP = np.reshape(XTrainNP, newshape=(256, 1, 14))\n",
    "# ohe = OneHotEncoder(sparse=False)\n",
    "# # to fix error, try this: https://www.youtube.com/watch?v=k_VAKyzggJI\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# lb = LabelEncoder()\n",
    "# yTrainOHE = lb.fit_transform(yTrain)\n",
    "# yTrainOHE = ohe.fit_transform(yTrain.to_numpy().reshape(-1,1))\n",
    "\n",
    "# ohe0 = OneHotEncoder(sparse=False)\n",
    "# yTestOHE = ohe0.fit(yTrain.to_numpy().reshape(-1,1)) # Daniel said to do this but I can't remember why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14876"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=64, kernel_size=1, activation='relu', input_shape=(256,14,1)))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Conv2D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Conv2D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "# model.add(LSTM(7, input_shape=(1,14,1), activation='tanh', \n",
    "#                recurrent_activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 256, 14, 64)       128       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 256, 14, 64)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 256, 14, 64)       4160      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 256, 14, 64)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 256, 14, 64)       4160      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 256, 14, 64)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 229376)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                7340064   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 11)                363       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,349,931\n",
      "Trainable params: 7,349,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-829e3f124d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "results = model.fit(XTrain, yTrainOHE, batch_size=20,epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best accuracy for the unprocessed signal with all channels was about .1020\n",
    "\n",
    "In 6/14 channels, adding SGF data was about the same\n",
    "\n",
    "In 6/14 channels, adding SGF and dropping raw data resulted in really low accuracy and NaN loss\n",
    "\n",
    "In 6/14 channels, ICA only had a accuracy of about .1000\n",
    "\n",
    "6/14 channels, unproccessed+SGF+ICA, 22,059 params: NaN loss again, what's wrong with the data? woe be the futility of mine ways\n",
    "\n",
    "6/14 channels, SGF only, 9,771 params, accuracy still around .1000\n",
    "\n",
    "6/14 channels, raw, 9,771 params, accuracy about .1000, so the .0020 increase was only from the extra channels. Processing doesn't seem to change the accuracy at all.\n",
    "\n",
    "----------\n",
    "Using vector data rows (\"wide\"), I get this error:\n",
    "```\n",
    "UnimplementedError                        Traceback (most recent call last)\n",
    "<ipython-input-59-f6344dc87bbc> in <module>\n",
    "      1 model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "----> 2 results = model.fit(XTrain, yTrainOHE, batch_size=20,epochs=30, verbose=1)\n",
    "\n",
    "...\n",
    "\n",
    "Cast string to float is not supported\n",
    "\t [[{{node sequential_4/Cast}}]] [Op:__inference_train_function_2630]\n",
    "```\n",
    "\n",
    "but the prep function seems to work fine: column is series, col[0] is list, and col[0][0] is float\n",
    "```\n",
    "(learn-env) :~/flatiron/cap/capstoneProject/workingNotebooks$ python dataPrepV2.py \n",
    "<class 'pandas.core.series.Series'> <class 'list'> <class 'float'>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-[] TODO: OHE test, validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-94abb3949cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             X, y = self._validate_data(X, y, dtype=np.float64,\n\u001b[0m\u001b[1;32m    161\u001b[0m                                        \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                                        accept_large_sparse=False)\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1781\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10234331754720463"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(XTest, yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick support vector classifier ended up getting the same accuracy, 0.10234331754720463"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Echo State Network Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 292650 into shape (48775,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-a01ff2f5544e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreadout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mridge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreservoir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m48775\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# train_statesy = reservoir.run(yTrain.to_numpy().reshape(-1, 1), reset=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mreadout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 292650 into shape (48775,1)"
     ]
    }
   ],
   "source": [
    "import reservoirpy as rpy\n",
    "from reservoirpy.nodes import Reservoir, Ridge, FORCE, ESN\n",
    "\n",
    "readout = Ridge(ridge=1e-7)\n",
    "train_states = reservoir.run(XTrain.to_numpy().reshape(-1, 1), reset=True)\n",
    "# train_statesy = reservoir.run(yTrain.to_numpy().reshape(-1, 1), reset=True)\n",
    "readout = readout.fit(train_states, yTrain.to_numpy().reshape(-1, 1), warmup=10)\n",
    "test_states = reservoir.run(XTest.to_numpy().reshape(-1, 1))\n",
    "yPred = readout.run(test_states)\n",
    "\n",
    "reservoir = Reservoir(100, lr=0.5, sr=0.9)\n",
    "ridge = Ridge(ridge=1e-7)\n",
    "\n",
    "esn_model = reservoir >> ridge\n",
    "\n",
    "esn_model = esn_model.fit(XTrain.to_numpy().reshape(-1, 1), yTrain.to_numpy().reshape(-1, 1).to_numpy().reshape(-1, 1), warmup=10)\n",
    "print(reservoir.is_initialized, readout.is_initialized, readout.fitted)\n",
    "\n",
    "yPred = esn_model.run(XTest.to_numpy().reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it was nice to learn how to set one of these up, I realized after a lot of tuning the reservoirpy library doesn't seem to have good interop with pandas. Might have interesting results with 1D timestamp and 1D value arrays, but not a multidimensional dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Reservoir-6: 100%|| 65034/65034 [00:06<00:00, 10457.73it/s]\n",
      "Running Reservoir-6: 100%|| 65034/65034 [00:06<00:00, 10306.03it/s]\n",
      "Running Model-7:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Running Model-7:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Running Model-7: 608it [00:00, 6077.69it/s]           \u001b[A\n",
      "Running Model-7: 1184it [00:00, 5977.77it/s]\u001b[A\n",
      "Running Model-7: 1774it [00:00, 5953.20it/s]\u001b[A\n",
      "Running Model-7: 2340it [00:00, 5861.04it/s]\u001b[A\n",
      "Running Model-7: 2958it [00:00, 5950.73it/s]\u001b[A\n",
      "Running Model-7: 3509it [00:00, 5809.06it/s]\u001b[A\n",
      "Running Model-7: 4114it [00:00, 5877.67it/s]\u001b[A\n",
      "Running Model-7: 4698it [00:00, 5864.64it/s]\u001b[A\n",
      "Running Model-7: 5266it [00:00, 5805.29it/s]\u001b[A\n",
      "Running Model-7: 5856it [00:01, 5832.72it/s]\u001b[A\n",
      "Running Model-7: 6438it [00:01, 5827.26it/s]\u001b[A\n",
      "Running Model-7: 7041it [00:01, 5886.27it/s]\u001b[A\n",
      "Running Model-7: 7640it [00:01, 5915.91it/s]\u001b[A\n",
      "Running Model-7: 8228it [00:01, 5904.99it/s]\u001b[A\n",
      "Running Model-7: 8820it [00:01, 5909.08it/s]\u001b[A\n",
      "Running Model-7: 9416it [00:01, 5922.98it/s]\u001b[A\n",
      "Running Model-7: 10023it [00:01, 5964.50it/s]\u001b[A\n",
      "Running Model-7: 10635it [00:01, 6009.18it/s]\u001b[A\n",
      "Running Model-7: 11236it [00:01, 5912.81it/s]\u001b[A\n",
      "Running Model-7: 11828it [00:02, 5818.73it/s]\u001b[A\n",
      "Running Model-7: 12411it [00:02, 5803.54it/s]\u001b[A\n",
      "Running Model-7: 12992it [00:02, 5790.53it/s]\u001b[A\n",
      "Running Model-7: 13572it [00:02, 5738.91it/s]\u001b[A\n",
      "Running Model-7: 14147it [00:02, 5703.06it/s]\u001b[A\n",
      "Running Model-7: 14731it [00:02, 5742.01it/s]\u001b[A\n",
      "Running Model-7: 15306it [00:02, 5728.33it/s]\u001b[A\n",
      "Running Model-7: 15890it [00:02, 5761.14it/s]\u001b[A\n",
      "Running Model-7: 16482it [00:02, 5805.01it/s]\u001b[A\n",
      "Running Model-7: 17063it [00:02, 5804.17it/s]\u001b[A\n",
      "Running Model-7: 17657it [00:03, 5843.72it/s]\u001b[A\n",
      "Running Model-7: 18258it [00:03, 5891.12it/s]\u001b[A\n",
      "Running Model-7: 18866it [00:03, 5944.25it/s]\u001b[A\n",
      "Running Model-7: 19461it [00:03, 5888.21it/s]\u001b[A\n",
      "Running Model-7: 20051it [00:03, 5788.12it/s]\u001b[A\n",
      "Running Model-7: 20644it [00:03, 5828.84it/s]\u001b[A\n",
      "Running Model-7: 21243it [00:03, 5873.62it/s]\u001b[A\n",
      "Running Model-7: 21831it [00:03, 5788.88it/s]\u001b[A\n",
      "Running Model-7: 22413it [00:03, 5795.93it/s]\u001b[A\n",
      "Running Model-7: 22999it [00:03, 5812.25it/s]\u001b[A\n",
      "Running Model-7: 23581it [00:04, 5633.60it/s]\u001b[A\n",
      "Running Model-7: 24158it [00:04, 5672.19it/s]\u001b[A\n",
      "Running Model-7: 24757it [00:04, 5762.30it/s]\u001b[A\n",
      "Running Model-7: 25335it [00:04, 5744.69it/s]\u001b[A\n",
      "Running Model-7: 25938it [00:04, 5825.64it/s]\u001b[A\n",
      "Running Model-7: 26541it [00:04, 5882.96it/s]\u001b[A\n",
      "Running Model-7: 27130it [00:04, 5822.68it/s]\u001b[A\n",
      "Running Model-7: 27713it [00:04, 5728.89it/s]\u001b[A\n",
      "Running Model-7: 28287it [00:04, 5676.58it/s]\u001b[A\n",
      "Running Model-7: 28866it [00:04, 5709.39it/s]\u001b[A\n",
      "Running Model-7: 29438it [00:05, 5634.17it/s]\u001b[A\n",
      "Running Model-7: 30002it [00:05, 5633.38it/s]\u001b[A\n",
      "Running Model-7: 30569it [00:05, 5643.93it/s]\u001b[A\n",
      "Running Model-7: 31137it [00:05, 5654.46it/s]\u001b[A\n",
      "Running Model-7: 31734it [00:05, 5743.76it/s]\u001b[A\n",
      "Running Model-7: 32327it [00:05, 5796.45it/s]\u001b[A\n",
      "Running Model-7: 32921it [00:05, 5836.26it/s]\u001b[A\n",
      "Running Model-7: 33511it [00:05, 5854.66it/s]\u001b[A\n",
      "Running Model-7: 34097it [00:05, 5827.35it/s]\u001b[A\n",
      "Running Model-7: 34680it [00:05, 5748.60it/s]\u001b[A\n",
      "Running Model-7: 35256it [00:06, 5632.46it/s]\u001b[A\n",
      "Running Model-7: 35821it [00:06, 5600.28it/s]\u001b[A\n",
      "Running Model-7: 36382it [00:06, 5593.31it/s]\u001b[A\n",
      "Running Model-7: 36969it [00:06, 5671.34it/s]\u001b[A\n",
      "Running Model-7: 37570it [00:06, 5768.24it/s]\u001b[A\n",
      "Running Model-7: 38164it [00:06, 5817.23it/s]\u001b[A\n",
      "Running Model-7: 38756it [00:06, 5845.81it/s]\u001b[A\n",
      "Running Model-7: 39342it [00:06, 5832.56it/s]\u001b[A\n",
      "Running Model-7: 39926it [00:06, 5831.38it/s]\u001b[A\n",
      "Running Model-7: 40519it [00:06, 5860.52it/s]\u001b[A\n",
      "Running Model-7: 41115it [00:07, 5887.87it/s]\u001b[A\n",
      "Running Model-7: 41711it [00:07, 5907.63it/s]\u001b[A\n",
      "Running Model-7: 42302it [00:07, 5756.75it/s]\u001b[A\n",
      "Running Model-7: 42879it [00:07, 5663.57it/s]\u001b[A\n",
      "Running Model-7: 43447it [00:07, 5589.41it/s]\u001b[A\n",
      "Running Model-7: 44007it [00:07, 5552.98it/s]\u001b[A\n",
      "Running Model-7: 44609it [00:07, 5684.41it/s]\u001b[A\n",
      "Running Model-7: 45214it [00:07, 5789.02it/s]\u001b[A\n",
      "Running Model-7: 45795it [00:07, 5662.03it/s]\u001b[A\n",
      "Running Model-7: 46363it [00:08, 5663.80it/s]\u001b[A\n",
      "Running Model-7: 46938it [00:08, 5688.58it/s]\u001b[A\n",
      "Running Model-7: 47533it [00:08, 5762.66it/s]\u001b[A\n",
      "Running Model-7: 48111it [00:08, 5678.11it/s]\u001b[A\n",
      "Running Model-7: 48680it [00:08, 5381.61it/s]\u001b[A\n",
      "Running Model-7: 49222it [00:08, 5390.64it/s]\u001b[A\n",
      "Running Model-7: 49811it [00:08, 5530.06it/s]\u001b[A\n",
      "Running Model-7: 50408it [00:08, 5654.45it/s]\u001b[A\n",
      "Running Model-7: 50979it [00:08, 5670.07it/s]\u001b[A\n",
      "Running Model-7: 51548it [00:08, 5660.56it/s]\u001b[A\n",
      "Running Model-7: 52116it [00:09, 5641.65it/s]\u001b[A\n",
      "Running Model-7: 52700it [00:09, 5694.78it/s]\u001b[A\n",
      "Running Model-7: 53271it [00:09, 5667.71it/s]\u001b[A\n",
      "Running Model-7: 53839it [00:09, 5644.05it/s]\u001b[A\n",
      "Running Model-7: 54416it [00:09, 5679.46it/s]\u001b[A\n",
      "Running Model-7: 55013it [00:09, 5760.96it/s]\u001b[A\n",
      "Running Model-7: 55619it [00:09, 5847.09it/s]\u001b[A\n",
      "Running Model-7: 56205it [00:09, 5736.22it/s]\u001b[A\n",
      "Running Model-7: 56780it [00:09, 5397.52it/s]\u001b[A\n",
      "Running Model-7: 57355it [00:09, 5496.57it/s]\u001b[A\n",
      "Running Model-7: 57940it [00:10, 5596.85it/s]\u001b[A\n",
      "Running Model-7: 58546it [00:10, 5726.75it/s]\u001b[A\n",
      "Running Model-7: 59122it [00:10, 5631.44it/s]\u001b[A\n",
      "Running Model-7: 59688it [00:10, 5584.71it/s]\u001b[A\n",
      "Running Model-7: 60274it [00:10, 5664.12it/s]\u001b[A\n",
      "Running Model-7: 60858it [00:10, 5714.58it/s]\u001b[A\n",
      "Running Model-7: 61450it [00:10, 5773.24it/s]\u001b[A\n",
      "Running Model-7: 62042it [00:10, 5813.29it/s]\u001b[A\n",
      "Running Model-7: 62625it [00:10, 5774.60it/s]\u001b[A\n",
      "Running Model-7: 63226it [00:10, 5841.09it/s]\u001b[A\n",
      "Running Model-7: 63832it [00:11, 5903.87it/s]\u001b[A\n",
      "Running Model-7: 64431it [00:11, 5928.65it/s]\u001b[A\n",
      "Running Model-7: 65034it [00:11, 5755.14it/s]\u001b[A\n",
      "Running Model-7: 100%|| 1/1 [00:11<00:00, 11.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node Ridge-19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Model-6: 1195it [00:00, 5902.90it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Model-6: 65034it [00:10, 6031.98it/s]\n"
     ]
    }
   ],
   "source": [
    "readout1 = Ridge(ridge=1e-7)\n",
    "train_states1 = reservoir.run(full['pfcAF4SGFdata'].to_numpy().reshape(-1, 1), reset=True)\n",
    "# train_statesy = reservoir.run(yTrain.to_numpy().reshape(-1, 1), reset=True)\n",
    "readout1 = readout1.fit(train_states1, full['code'].to_numpy().reshape(-1, 1), warmup=10)\n",
    "test_states1 = reservoir.run((full['pfcAF4SGFdata'].to_numpy().reshape(-1, 1)))\n",
    "# yPred1 = readout.run(test_states1)\n",
    "\n",
    "reservoir1 = Reservoir(100, lr=0.5, sr=0.9)\n",
    "ridge1 = Ridge(ridge=1e-7)\n",
    "\n",
    "esn_model1 = reservoir >> ridge\n",
    "\n",
    "esn_model1 = esn_model1.fit(full['pfcAF4SGFdata'].to_numpy().reshape(-1, 1), \n",
    "                             full['code'].to_numpy().reshape(-1, 1) ,warmup=10)\n",
    "print(reservoir.is_initialized, readout.is_initialized, readout.fitted)\n",
    "\n",
    "yPred1 = esn_model.run((full['pfcAF4SGFdata'].to_numpy().reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4.494217014863551}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(yPred1.flatten()), (mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO use exploded data of one event from original CSV, predict brain activity with ESN:\n",
    "# forecast, generate artificial brainwaves of looking at a digit\n",
    "\n",
    "# TODO explore possibility of using matplotlib plots of signals as training data\n",
    "# like in https://arxiv.org/abs/1511.06448\n",
    "\n",
    "# in the future, try iterating ESNpredict -> ESNtrain -> ESNpredict -> ESNtrain -> ...\n",
    "# to see how predictions change over time, maybe there would be a \"core\" shape that can be\n",
    "# like a waveform decomposition of the number, \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
