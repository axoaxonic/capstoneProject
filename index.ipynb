{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               total        used        free      shared  buff/cache   available\r\n",
      "Mem:            15Gi        10Gi       277Mi       451Mi       4.9Gi       4.4Gi\r\n",
      "Swap:          976Mi       181Mi       795Mi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM STATEMENT & MOTIVATION\n",
    "-----------------\n",
    "-----------------\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Brain-Computer Interfaces (BCIs), can be extremely empowering for people with disabilities who choose to use them. There are a few commercially available EEG headsets, such as the Emotiv EPOC (pictured below) that provide less cost prohibitive, noninvasive options to convert electrical signals from the brain into computer commands that might be otherwise inaccessible to input for someone.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "![imageof the Emotiv EPOC+ headset on a white background, next to a schematic of the 10-20 electrode placement system](https://d2z0k1elb7rxgj.cloudfront.net/uploads/2018/11/a-Emotiv-EPOC-headset-b-Spatial-mapping-of-the-electrodes-on-the-scalp.jpg)\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "This project aims to use some techniques from the field of data science to explore the feasibility of classiying EEG signals captured by a low cost, dry electrode system such as the Emotiv EPOC+. The data used was collected over nearly two years, 2014-15 and is [curated and hosted by the subject of the readings, David Vivancos](http://mindbigdata.com/opendb/index.html). Although four different datasets using four different devices are available, for this project I decided to analyze the one with the most channels, especially since it was the only one with electrode channels on the occipital lobe, which is the \"visual cortex.\" Lower cost options are available, such using a development board or microcontroller (\\\\$5 - \\\\$50) with an amplifier such as [this one](https://biosignals.berndporr.me.uk/#build_your_own_bio-amplifier), with electrodes from any supplier, which can be only [a few dollars](https://www.alibaba.com/product-detail/Colorful-Reusable-Gold-Cup-Electrodes-Cable_1600592681920.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import scipy\n",
    "from scipy.signal import hilbert, savgol_filter, wavelets, periodogram\n",
    "# from sklearn.decomposition import FastICA\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Conv2D, GlobalMaxPooling2D, MaxPooling2D, Flatten\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PROCESSING\n",
    "-----------------\n",
    "-----------------\n",
    "\n",
    "Uncomment and run the cell below to load and process the data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load in original raw data and give it column names\n",
    "# cols = ['id', 'event', 'device', 'channel', 'code', 'size', 'data']\n",
    "# emotiv = pd.read_csv('../../fulldata/EP1.txt', delimiter='\\t', names=cols)\n",
    "# emotiv.drop(['device', 'id'], inplace=True, axis=1)\n",
    "\n",
    "# # Break up full df into sub-dfs by channel\n",
    "# occO1 = emotiv[(emotiv['channel'] == 'O1')]\n",
    "# occO2 = emotiv[(emotiv['channel'] == 'O2')]\n",
    "# fefF3 = emotiv[(emotiv['channel'] == 'F3')]\n",
    "# fefF4 = emotiv[(emotiv['channel'] == 'F4')]\n",
    "# fefF7 = emotiv[(emotiv['channel'] == 'F7')]\n",
    "# fefF8 = emotiv[(emotiv['channel'] == 'F8')]\n",
    "# temT7 = emotiv[(emotiv['channel'] == 'T7')]\n",
    "# temT8 = emotiv[(emotiv['channel'] == 'T8')]\n",
    "# pfcAF3 = emotiv[(emotiv['channel'] == 'AF3')]\n",
    "# pfcAF4 = emotiv[(emotiv['channel'] == 'AF4')]\n",
    "# motFC5 = emotiv[(emotiv['channel'] == 'FC5')]\n",
    "# motFC6 = emotiv[(emotiv['channel'] == 'FC6')]\n",
    "# parP7 = emotiv[(emotiv['channel'] == 'P7')]\n",
    "# parP8 = emotiv[(emotiv['channel'] == 'P8')]\n",
    "\n",
    "# # Delete and garbage collect the full df so computer doesn't run out of RAM and freeze\n",
    "# del emotiv\n",
    "# gc.collect()\n",
    "\n",
    "# def dataProcessor(df):\n",
    "#     '''\n",
    "# Cleans data column by splitting it into smaller strings, converting those to float, cutting it down to length defined by shortest data vector, normalizing the indexes by resetting.\n",
    "\n",
    "# i: Dataframe for single channel\n",
    "# o: Processed dataframe, printouts of lengths before and after clipping for check, timestamp for each iteration\n",
    "#     '''\n",
    "    \n",
    "#     col = df['data'].apply(lambda x: list(map(float, x.split(','))))\n",
    "#     print(type(col), type(col.iloc[0]), type(col.iloc[0][0]))\n",
    "\n",
    "#     for i in range(len(col)):\n",
    "#         l = []\n",
    "#         l.append(len(col.iloc[i]))\n",
    "\n",
    "#     print(min(l))\n",
    "\n",
    "#     for i in range(len(col)):\n",
    "#         col.iloc[i] = col.iloc[i][:256] # or 257?\n",
    "\n",
    "#     for i in range(len(col)):\n",
    "#         l = []\n",
    "#         l.append(len(col.iloc[i]))\n",
    "\n",
    "#     print(max(l))\n",
    "#     return col.reset_index(drop=True)\n",
    "\n",
    "# # Choose  which channels to include\n",
    "# dfs = [occ0, occ1, fefF3,\n",
    "#        fefF4, fefF7, fefF8, temT7,temT8,\n",
    "#         pfcAF3, pfcAF4, motFC5, motFC6,\n",
    "#         parP7, parP8]\n",
    "\n",
    "# # Init blank dataframe for processed channels to be added to\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# #  select columns by name by grabbing channel name value string from the 'channel' column\n",
    "# # then running dataProcessor on each/any channel dataframes\n",
    "# for x in dfs:\n",
    "#     name = x['channel'].iloc[0]\n",
    "#     df[name] = dataProcessor(x) \n",
    "#     print(time.time())\n",
    "\n",
    "# # Add code column from any channel df\n",
    "# df['code'] = occ1['code'].reset_index(drop=True)\n",
    "# print(df.head())\n",
    "# print(type(df), type(df.iloc[0]), type(df.iloc[0][0]))\n",
    "\n",
    "# # Delete original dfs with this ugly stack of dels, garbage collect to conserve RAM\n",
    "# del occ0\n",
    "# del occ1\n",
    "# del fefF3\n",
    "# del fefF4\n",
    "# del fefF7\n",
    "# del fefF8\n",
    "# del temT7\n",
    "# del temT8\n",
    "# del pfcAF3\n",
    "# del pfcAF4\n",
    "# del motFC5\n",
    "# del motFC6\n",
    "# del parP7\n",
    "# del parP8\n",
    "# gc.collect()\n",
    "\n",
    "# # Save resulting dataframe to csv\n",
    "# df.to_csv('../data/df.csv', sep=';', quoting=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[3989.23077, 3983.589744, 3987.692308, 3991.794872, 3992.307693, 3990.256411, 3993.333334, 4000.0, 3994.871795, 3985.128206, 3995.897436, 4001.538461, 3986.153847, 3982.051283, 3990.256411, 3994.358975, 3981.538462, 3961.025642, 3968.205129, 3989.23077, 3980.512821, 3973.333334, 3997.948718, 4009.230769, 3998.974359, 3998.461539, 3999.48718, 3997.435898, 3999.48718, 4001.025641, 3994.871795, 3987.179488, 3987.692308, 3992.307693, 3987.692308, 3980.0, 3984.102565, 3994.358975, 3991.794872, 3982.564103, 3980.0, 3978.461539, 3978.461539, 3989.23077, 3998.461539, 3998.461539, 3996.410257, 3978.974359, 3966.153847, 3978.974359, 3987.692308, 3985.128206, 3993.846154, 3995.384616, 3974.871795, 3957.948718, 3965.128206, 3990.769231, 4007.179487, 3998.974359, 3981.025642, 3970.256411, 3976.923077, 3993.333334, 3992.820513, 3978.461539, 3976.923077, 3975.897436, 3973.846154, 3984.615385, 3986.153847, 3978.974359, 3980.0, 3982.051283, 3981.538462, 3983.589744, 3978.974359, 3971.282052, 3972.820513, 3971.794872, 3969.23077, 3976.923077, 3975.897436, 3967.692308, 3972.820513, 3984.102565, 3983.589744, 3974.871795, 3967.692308, 3966.153847, 3974.358975, 3981.025642, 3971.282052, 3963.076924, 3973.846154, 3986.153847, 3983.076924, 3981.025642, 3986.666667, 3984.615385, 3977.435898, 3975.384616, 3974.358975, 3973.333334, 3978.461539, 3983.589744, 3984.615385, 3990.256411, 3988.205129, 3974.358975, 3983.589744, 4000.51282, 3986.153847, 3969.23077, 3979.48718, 3987.692308, 3980.512821, 3980.512821, 3994.358975, 4001.538461, 3988.717949, 3981.025642, 3992.820513, 3996.923077, 3988.717949, 3981.538462, 3973.846154, 3970.769231, 3982.051283, 3998.461539, 3999.48718, 3989.74359, 3984.615385, 3989.23077, 3988.205129, 3971.282052, 3969.74359, 3990.256411, 3990.769231, 3967.692308, 3963.076924, 3978.974359, 3987.692308, 3981.538462, 3974.871795, 3986.666667, 3994.871795, 3980.512821, 3975.384616, 3986.153847, 3984.615385, 3980.512821, 3985.641026, 3986.153847, 3980.512821, 3986.153847, 3991.794872, 3978.461539, 3970.256411, 3985.128206, 3989.23077, 3982.051283, 3987.692308, 3990.769231, 3982.564103, 3989.74359, 4000.0, 3985.641026, 3973.333334, 3989.74359, 4012.307692, 4016.923076, 4009.743589, 4008.717948, 4010.76923, 4005.641025, 3998.461539, 3997.435898, 3997.948718, 3994.871795, 3992.307693, 3991.282052, 3989.23077, 3990.256411, 3994.358975, 3997.948718, 4003.589743, 4008.717948, 4008.717948, 3998.974359, 3985.128206, 3985.128206, 3996.923077, 3999.48718, 3990.256411, 3983.589744, 3982.564103, 3981.025642, 3981.025642, 3988.717949, 3990.769231, 3986.666667, 3996.410257, 4000.51282, 3987.692308, 3982.564103, 3986.153847, 3990.769231, 4001.538461, 4006.153846, 3994.358975, 3982.051283, 3986.153847, 4000.0, 3999.48718, 3986.666667, 3985.128206, 3991.794872, 3996.923077, 4004.102564, 3997.948718, 3985.128206, 3994.871795, 4007.179487, 3995.897436, 3991.282052, 4007.692307, 4013.846153, 4003.076923, 3994.871795, 3997.948718, 4008.717948, 4013.846153, 4005.641025, 3995.384616, 3993.846154, 3999.48718, 4006.666666, 4008.717948, 4004.615384, 3999.48718, 3993.333334, 3993.333334, 4000.51282, 4001.538461, 3997.948718, 3999.48718, 4001.025641, 3996.410257, 3996.923077, 3997.435898, 3994.358975, 3996.410257, 4001.025641, 4005.641025, 4014.871794]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading dataset if saved before\n",
    "df = pd.read_csv('./data/dfTensor.csv', delimiter=';', encoding='latin-1') # This line might show an older version's filename, dfTensor\n",
    "df.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "\n",
    "#Check arbitrary column to see if the processing step encoded the data vectors as strings, which it probably did:\n",
    "df['F8'].iloc[0], df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If modeling a single channel, run this cell\n",
    "\n",
    "dfO1 = df['O1'] #.explode?\n",
    "dfO1.applymap(eval)\n",
    "dfO1.explode # or to numpy array flatten back to df?\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SETUP\n",
    "-----------------\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up X and y\n",
    "X = df.drop('code', axis=1)\n",
    "y = df['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to convert the values to numerical type before signal processing.\n",
    "# X = X.applymap(eval) # takes a while\n",
    "# X = X.applymap(np.array) # try this, maybe at another place, to hopefully fix the shape issue\n",
    "# print(X.iloc[0], type(y.iloc[0]), X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split for \"df\"\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain.shape, XTest.shape, yTrain.shape, yTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainNP = XTrain.applymap(np.array)\n",
    "XTrainNP = XTrainNP.to_numpy()\n",
    "XTrainNP.shape\n",
    "XTrainNP = np.reshape(XTrainNP, newshape=(256, 1, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIGNAL PREPROCESSING\n",
    "-----------------\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Savitzky-Golay Filter\n",
    "\n",
    "![Gif of how the filter smoothly approximates the curve at discrete time steps](https://upload.wikimedia.org/wikipedia/commons/8/89/Lissage_sg3_anim.gif)\n",
    "\n",
    "\"The idea of Savitzky-Golay filters is simple â€“ for each sample in the filtered sequence, take its direct neighborhood of N neighbors and fit a polynomial to it. Then just evaluate the polynomial at its center (and the center of the neighborhood), point 0, and continue with the next neighborhood. \"\n",
    "\n",
    "\n",
    "-- https://bartwronski.com/2021/11/03/study-of-smoothing-filters-savitzky-golay-filters/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XSGF = X[X[['O1', 'O2', 'F3', 'F4', 'F7', 'F8', 'T7', 'T8', 'AF3', 'AF4', 'FC5',\n",
    "       'FC6', 'P7', 'P8']]].applymap(savgol_filter( 10001, 1 ) )\n",
    "#dfSGF.concatenate(df['code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast ICA\n",
    "\n",
    "![Logo of the name of the algorithm from its project website from the University of Aalto, Finland ](https://research.ics.aalto.fi/ica/fastica/FastICA.gif)\n",
    "\n",
    "\"Independent component analysis (ICA) is a statistical and computational technique for revealing hidden factors that underlie sets of random variables, measurements, or signals.\n",
    "\n",
    "ICA defines a generative model for the observed multivariate data, which is typically given as a large database of samples. In the model, the data variables are assumed to be linear mixtures of some unknown latent variables, and the mixing system is also unknown. The latent variables are assumed nongaussian and mutually independent, and they are called the independent components of the observed data. These independent components, also called sources or factors, can be found by ICA. \"\n",
    "\n",
    "\n",
    "-- https://www.cs.helsinki.fi/u/ahyvarin/whatisica.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fICA = FastICA(5, whiten=True)\n",
    "XfICA = fICA.fit_transform(X.to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING\n",
    "-----------------\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate and setup the model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size=1, activation='relu', input_shape=(256,14,1)))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Conv2D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Conv2D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "# Print summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most recent model summary:\n",
    "\n",
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " conv2d (Conv2D)             (None, 256, 14, 64)       128       \n",
    "                                                                 \n",
    " max_pooling2d (MaxPooling2D  (None, 256, 14, 64)      0         \n",
    " )                                                               \n",
    "                                                                 \n",
    " conv2d_1 (Conv2D)           (None, 256, 14, 64)       4160      \n",
    "                                                                 \n",
    " max_pooling2d_1 (MaxPooling  (None, 256, 14, 64)      0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " conv2d_2 (Conv2D)           (None, 256, 14, 64)       4160      \n",
    "                                                                 \n",
    " max_pooling2d_2 (MaxPooling  (None, 256, 14, 64)      0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " flatten (Flatten)           (None, 229376)            0         \n",
    "                                                                 \n",
    " dense (Dense)               (None, 32)                7340064   \n",
    "                                                                 \n",
    " dense_1 (Dense)             (None, 32)                1056      \n",
    "                                                                 \n",
    " dense_2 (Dense)             (None, 11)                363       \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 7,349,931\n",
    "Trainable params: 7,349,931\n",
    "Non-trainable params: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run compile and fit model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "results = model.fit(XTrain, yTrainOHE, batch_size=20,epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a quck Support Vector Classification to check if there is similar performance on a simpler model\n",
    "svc = SVC()\n",
    "svc.fit(X, y)\n",
    "\n",
    "svc.score(XTest, yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS\n",
    "---------------\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first iterations of the model were using data that was averaged per event. This effectively erased the time data, which is apparently crucial for neural coding (see this [video lecture by Earl Miller for more information on why](https://www.youtube.com/watch?v=Kqyhr9fTUjs)). This seems like it would be made worse by raw data being a measurement of changes in amlitude over time, even with filters. It might perform better the frequency information from Fourier transformation, but mean freqency without time would still lose a lot of information, other than perhaps the dominant frequency band of the channel. \n",
    "\n",
    "\n",
    ">\n",
    ">Best accuracy for the unprocessed signal with all channels was about .1020\n",
    "> &nbsp;\n",
    ">\n",
    ">In 6/14 channels, adding SGF data was about the same\n",
    ">\n",
    "> &nbsp;\n",
    ">\n",
    ">In 6/14 channels, adding SGF and dropping raw data resulted in really low accuracy and NaN loss\n",
    ">\n",
    "> &nbsp;\n",
    ">\n",
    ">In 6/14 channels, ICA only had a accuracy of about .1000\n",
    ">\n",
    "> &nbsp;\n",
    ">\n",
    ">6/14 channels, unproccessed+SGF+ICA, 22,059 params: NaN loss again, what's wrong with the data? woe be the futility of mine ways\n",
    ">\n",
    "> &nbsp;\n",
    ">\n",
    ">6/14 channels, SGF only, 9,771 params, accuracy still around .1000\n",
    ">\n",
    "> &nbsp;\n",
    ">\n",
    ">6/14 channels, raw, 9,771 params, accuracy about .1000, so the .0020 increase was only from the extra channels. Processing doesn't seem to change the accuracy at all.\n",
    ">\n",
    "> &nbsp;\n",
    ">\n",
    " ----------\n",
    "\n",
    " &nbsp;\n",
    "\n",
    "\n",
    "Realizing the temporal coding is crucial, I ran the model using vector data rows (\"wide\"). The first iterations of this process indicated that the TF CNN algorithm cannot run on matrices whos values are both scalars and vectors. To correct this, I need to find a way to project it to a lower dimension while preserving the order of the readings. This is currently in process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIBLIOGRAPHY\n",
    "---------------\n",
    "---------------\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- \"Deep learning for electroencephalogram (EEG) classification tasks: a review\", Alexander Craik et al 2019 J. Neural Eng. 16 031001, doi:10.1088/1741-2552/ab0ab5\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
