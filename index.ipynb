{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM STATEMENT & MOTIVATION\n",
    "-----------------\n",
    "-----------------\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Brain-Computer Interfaces (BCIs), can be extremely empowering for people with disabilities who choose to use them. There are a few commercially available EEG headsets, such as the Emotiv EPOC (pictured below) that provide less cost prohibitive, noninvasive options to convert electrical signals from the brain into computer commands that might be otherwise inaccessible to input for someone.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "![imageof the Emotiv EPOC+ headset on a white background, next to a schematic of the 10-20 electrode placement system](https://d2z0k1elb7rxgj.cloudfront.net/uploads/2018/11/a-Emotiv-EPOC-headset-b-Spatial-mapping-of-the-electrodes-on-the-scalp.jpg)\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "This project aims to use some techniques from the field of data science to explore the feasibility of classiying EEG signals captured by a low cost, dry electrode system such as the Emotiv EPOC+. The data used was collected over nearly two years, 2014-15 and is [curated and hosted by the subject of the readings, David Vivancos](http://mindbigdata.com/opendb/index.html). Although four different datasets using four different devices are available, for this project I decided to analyze the one with the most channels, especially since it was the only one with electrode channels on the occipital lobe, which is the \"visual cortex.\" Lower cost options are available, such using a development board or microcontroller (\\\\$5 - \\\\$50) with an amplifier such as [this one](https://biosignals.berndporr.me.uk/#build_your_own_bio-amplifier), with electrodes from any supplier, which can be only [a few dollars](https://www.alibaba.com/product-detail/Colorful-Reusable-Gold-Cup-Electrodes-Cable_1600592681920.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import itertools\n",
    "import scipy\n",
    "from scipy.signal import hilbert, savgol_filter, wavelets, periodogram\n",
    "from sklearn.decomposition import FastICA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, GlobalMaxPooling2D, MaxPooling2D, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PROCESSING\n",
    "-----------------\n",
    "-----------------\n",
    "\n",
    "Uncomment and run the cell below to load and process the data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load in original raw data and give it column names\n",
    "cols = ['id', 'event', 'device', 'channel', 'code', 'size', 'data']\n",
    "emotiv = pd.read_csv('../../fulldata/EP1.txt', delimiter='\\t', names=cols)\n",
    "emotiv.drop(['device', 'id'], inplace=True, axis=1)\n",
    "\n",
    "# Optional: break up full df into sub-dfs by channel\n",
    "# occO1 = emotiv[(emotiv['channel'] == 'O1')]\n",
    "# occO2 = emotiv[(emotiv['channel'] == 'O2')]\n",
    "# fefF3 = emotiv[(emotiv['channel'] == 'F3')]\n",
    "# fefF4 = emotiv[(emotiv['channel'] == 'F4')]\n",
    "# fefF7 = emotiv[(emotiv['channel'] == 'F7')]\n",
    "# fefF8 = emotiv[(emotiv['channel'] == 'F8')]\n",
    "# temT7 = emotiv[(emotiv['channel'] == 'T7')]\n",
    "# temT8 = emotiv[(emotiv['channel'] == 'T8')]\n",
    "# pfcAF3 = emotiv[(emotiv['channel'] == 'AF3')]\n",
    "# pfcAF4 = emotiv[(emotiv['channel'] == 'AF4')]\n",
    "# motFC5 = emotiv[(emotiv['channel'] == 'FC5')]\n",
    "# motFC6 = emotiv[(emotiv['channel'] == 'FC6')]\n",
    "# parP7 = emotiv[(emotiv['channel'] == 'P7')]\n",
    "# parP8 = emotiv[(emotiv['channel'] == 'P8')]\n",
    "\n",
    "# # Delete and garbage collect the full df so computer doesn't run out of RAM and freeze\n",
    "# del emotiv\n",
    "# gc.collect()\n",
    "\n",
    "def dataProcessor(df):\n",
    "    '''\n",
    "Cleans data column by splitting it into smaller strings, converting those to float, cutting it down to length defined by shortest data vector, normalizing the indexes by resetting.\n",
    "\n",
    "i: Dataframe for single channel\n",
    "o: Processed dataframe, printouts of lengths before and after clipping for check, timestamp for each iteration\n",
    "    '''\n",
    "    \n",
    "    col = df['data'].apply(lambda x: list(map(float, x.split(','))))\n",
    "    print(type(col), type(col.iloc[0]), type(col.iloc[0][0]))\n",
    "\n",
    "    for i in range(len(col)):\n",
    "        l = []\n",
    "        l.append(len(col.iloc[i]))\n",
    "\n",
    "    print(min(l))\n",
    "\n",
    "    for i in range(len(col)):\n",
    "        col.iloc[i] = col.iloc[i][:256] # or 257?\n",
    "\n",
    "    for i in range(len(col)):\n",
    "        l = []\n",
    "        l.append(len(col.iloc[i]))\n",
    "\n",
    "    print(max(l))\n",
    "    return col.reset_index(drop=True)\n",
    "\n",
    "# Choose  which channels to include\n",
    "dfs = [occO1, occO2, fefF3,\n",
    "       fefF4, fefF7, fefF8, temT7,temT8,\n",
    "        pfcAF3, pfcAF4, motFC5, motFC6,\n",
    "        parP7, parP8]\n",
    "\n",
    "# Init blank dataframe for processed channels to be added to\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#  select columns by name by grabbing channel name value string from the 'channel' column\n",
    "# then running dataProcessor on each/any channel dataframes\n",
    "for x in dfs:\n",
    "    name = x['channel'].iloc[0]\n",
    "    df[name] = dataProcessor(x) \n",
    "\n",
    "# Add code column from any channel df\n",
    "df['code'] = occO1['code'].reset_index(drop=True)\n",
    "print(df.head())\n",
    "print(type(df), type(df.iloc[0]), type(df.iloc[0][0]))\n",
    "\n",
    "# Delete original dfs with this ugly stack of dels, garbage collect to conserve RAM\n",
    "del occO1\n",
    "del occO2\n",
    "del fefF3\n",
    "del fefF4\n",
    "del fefF7\n",
    "del fefF8\n",
    "del temT7\n",
    "del temT8\n",
    "del pfcAF3\n",
    "del pfcAF4\n",
    "del motFC5\n",
    "del motFC6\n",
    "del parP7\n",
    "del parP8\n",
    "gc.collect()\n",
    "\n",
    "# # Save resulting dataframe to csv\n",
    "# df.to_csv('../data/df.csv', sep=';', quoting=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading each channel then merging to a single dataframe\n",
    "\n",
    "#Note: these names are \n",
    "occO0 = pd.read_csv('../data/occ0Exp.csv', delimiter=',')\n",
    "occO1 = pd.read_csv('../data/occ1Exp.csv', delimiter=',')\n",
    "fefF3 = pd.read_csv('../data/fefF3Exp.csv', delimiter=',')\n",
    "fefF4 = pd.read_csv('../data/fefF4Exp.csv', delimiter=',')\n",
    "fefF7 = pd.read_csv('../data/fefF7Exp.csv', delimiter=',')\n",
    "fefF8 = pd.read_csv('../data/fefF8Exp.csv', delimiter=',')\n",
    "temT7 = pd.read_csv('../data/temT7Exp.csv', delimiter=',')\n",
    "temT8 = pd.read_csv('../data/temT8Exp.csv', delimiter=',')\n",
    "pfcAF3 = pd.read_csv('../data/pfcAF3Exp.csv', delimiter=',')\n",
    "pfcAF4 = pd.read_csv('../data/pfcAF4Exp.csv', delimiter=',')\n",
    "motFC5 = pd.read_csv('../data/motFC5Exp.csv', delimiter=',')\n",
    "motFC6 = pd.read_csv('../data/motFC6Exp.csv', delimiter=',')\n",
    "parP7 = pd.read_csv('../data/parP7Exp.csv', delimiter=',')\n",
    "parP8 = pd.read_csv('../data/parP8Exp.csv', delimiter=',')\n",
    "\n",
    "# OPTIONAL: load same datasets expanded with filter processing, as shown below (saving process in working code)\n",
    "\n",
    "# occO0 = pd.read_csv('../data/occ0Proc.csv', delimiter=',')\n",
    "# occO1 = pd.read_csv('../data/occ1Proc.csv', delimiter=',') \n",
    "# fefF3 = pd.read_csv('../data/fefF3Proc.csv', delimiter=',')\n",
    "# fefF4 = pd.read_csv('../data/fefF4Proc.csv', delimiter=',')\n",
    "# fefF7 = pd.read_csv('../data/fefF7Proc.csv', delimiter=',')\n",
    "# fefF8 = pd.read_csv('../data/fefF8Proc.csv', delimiter=',') \n",
    "# temT7 = pd.read_csv('../data/temT7Proc.csv', delimiter=',')\n",
    "# temT8 = pd.read_csv('../data/temT8Proc.csv', delimiter=',')\n",
    "# pfcAF3 = pd.read_csv('../data/pfcAF3Proc.csv', delimiter=',')\n",
    "# pfcAF4 = pd.read_csv('../data/pfcAF4Proc.csv', delimiter=',')\n",
    "# motFC5 = pd.read_csv('../data/motFC5Proc.csv', delimiter=',')\n",
    "# motFC6 = pd.read_csv('../data/motFC6Proc.csv', delimiter=',')\n",
    "# parP7 = pd.read_csv('../data/parP7Proc.csv', delimiter=',')\n",
    "# parP8 = pd.read_csv('../data/parP8Proc.csv', delimiter=',')\n",
    "\n",
    "\n",
    "#Rename all the columns so they are unique when concatenated \n",
    "occO0.rename(columns={'data': 'occ0Data'}, inplace=True)\n",
    "occO1.rename(columns={'data': 'occ1Data'}, inplace=True)\n",
    "fefF3.rename(columns={'data': 'fefF3Data'}, inplace=True)\n",
    "fefF4.rename(columns={'data': 'fefF4Data'}, inplace=True)\n",
    "fefF7.rename(columns={'data': 'fefF7Data'}, inplace=True)\n",
    "fefF8.rename(columns={'data': 'fefF8Data'}, inplace=True)\n",
    "temT7.rename(columns={'data': 'temT7Data'}, inplace=True)\n",
    "temT8.rename(columns={'data': 'temT8Data'}, inplace=True)\n",
    "pfcAF3.rename(columns={'data': 'pfcAF3Data'}, inplace=True)\n",
    "pfcAF4.rename(columns={'data': 'pfcAF4Data'}, inplace=True)\n",
    "motFC5.rename(columns={'data': 'motFC5Data'}, inplace=True)\n",
    "motFC6.rename(columns={'data': 'motFC6Data'}, inplace=True)\n",
    "parP7.rename(columns={'data': 'parP7Data'}, inplace=True)\n",
    "parP8.rename(columns={'data': 'parP8Data'}, inplace=True)\n",
    "\n",
    "\n",
    "# Function to process the channels and average the data vectors by event\n",
    "\n",
    "for i in dfs:\n",
    "    i.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "    i['data'].astype(float, copy=False)\n",
    "    print(i.columns)\n",
    "    i = i.groupby('event').mean()\n",
    "    print(i.columns)\n",
    "\n",
    "    \n",
    "# Merge into one dataframe\n",
    "#merge all DataFrames into one\n",
    "df = reduce(lambda  left,right: pd.merge(left,right,on=['event'],\n",
    "                                            how='outer'), dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SETUP\n",
    "-----------------\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up X and y\n",
    "X = df.drop('code', axis=1)\n",
    "y = df['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to convert the values to numerical type before signal processing.\n",
    "X = X.applymap(eval) # takes a while\n",
    "print(X.iloc[0], type(y.iloc[0]), X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split for \"df\"\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XTrain.shape, XTest.shape, yTrain.shape, yTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainNP = XTrain.applymap(np.array)\n",
    "XTrainNP = XTrainNP.to_numpy()\n",
    "XTrainNP.shape\n",
    "XTrainNP = np.reshape(XTrainNP, newshape=(256, 14, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encode the y values\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "yTrainOHE = ohe.fit_transform(yTrain.to_numpy().reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIGNAL PREPROCESSING\n",
    "-----------------\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Savitzky-Golay Filter\n",
    "\n",
    "![Gif of how the filter smoothly approximates the curve at discrete time steps](https://upload.wikimedia.org/wikipedia/commons/8/89/Lissage_sg3_anim.gif)\n",
    "\n",
    "\"The idea of Savitzky-Golay filters is simple – for each sample in the filtered sequence, take its direct neighborhood of N neighbors and fit a polynomial to it. Then just evaluate the polynomial at its center (and the center of the neighborhood), point 0, and continue with the next neighborhood. \"\n",
    "\n",
    "\n",
    "-- https://bartwronski.com/2021/11/03/study-of-smoothing-filters-savitzky-golay-filters/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XSGF = X[X[['O1', 'O2', 'F3', 'F4', 'F7', 'F8', 'T7', 'T8', 'AF3', 'AF4', 'FC5',\n",
    "       'FC6', 'P7', 'P8']]].applymap(savgol_filter(10001, 1))\n",
    "#dfSGF.concatenate(df['code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast ICA\n",
    "\n",
    "![Logo of the name of the algorithm from its project website from the University of Aalto, Finland ](https://research.ics.aalto.fi/ica/fastica/FastICA.gif)\n",
    "\n",
    "\"Independent component analysis (ICA) is a statistical and computational technique for revealing hidden factors that underlie sets of random variables, measurements, or signals.\n",
    "\n",
    "ICA defines a generative model for the observed multivariate data, which is typically given as a large database of samples. In the model, the data variables are assumed to be linear mixtures of some unknown latent variables, and the mixing system is also unknown. The latent variables are assumed nongaussian and mutually independent, and they are called the independent components of the observed data. These independent components, also called sources or factors, can be found by ICA. \"\n",
    "\n",
    "\n",
    "-- https://www.cs.helsinki.fi/u/ahyvarin/whatisica.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fICA = FastICA(5, whiten=True)\n",
    "XfICA = fICA.fit_transform(X.to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING\n",
    "-----------------\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate and setup the model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size=1, activation='relu', input_shape=(256,14,1)))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Conv2D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Conv2D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "# Print summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most recent model summary:\n",
    "\n",
    "```\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " conv2d (Conv2D)             (None, 256, 14, 64)       128       \n",
    "                                                                 \n",
    " max_pooling2d (MaxPooling2D  (None, 256, 14, 64)      0         \n",
    " )                                                               \n",
    "                                                                 \n",
    " conv2d_1 (Conv2D)           (None, 256, 14, 64)       4160      \n",
    "                                                                 \n",
    " max_pooling2d_1 (MaxPooling  (None, 256, 14, 64)      0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " conv2d_2 (Conv2D)           (None, 256, 14, 64)       4160      \n",
    "                                                                 \n",
    " max_pooling2d_2 (MaxPooling  (None, 256, 14, 64)      0         \n",
    " 2D)                                                             \n",
    "                                                                 \n",
    " flatten (Flatten)           (None, 229376)            0         \n",
    "                                                                 \n",
    " dense (Dense)               (None, 32)                7340064   \n",
    "                                                                 \n",
    " dense_1 (Dense)             (None, 32)                1056      \n",
    "                                                                 \n",
    " dense_2 (Dense)             (None, 11)                363       \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 7,349,931\n",
    "Trainable params: 7,349,931\n",
    "Non-trainable params: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run compile and fit model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "results = model.fit(XTrain, yTrainOHE, batch_size=20,epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve the score, the vectors of each row of the data column are expanded out into columns\n",
    "XO1 = pd.DataFrame(X['O1'].tolist())\n",
    "XO2 = pd.DataFrame(X['O2'].tolist())\n",
    "XF3 = pd.DataFrame(X['F3'].tolist())\n",
    "XF4 = pd.DataFrame(X['F4'].tolist())\n",
    "XF7 = pd.DataFrame(X['F7'].tolist())\n",
    "XF8 = pd.DataFrame(X['F8'].tolist())\n",
    "XT7 = pd.DataFrame(X['T7'].tolist())\n",
    "XT8 = pd.DataFrame(X['T8'].tolist())\n",
    "XAF3 = pd.DataFrame(X['AF3'].tolist())\n",
    "XAF4 = pd.DataFrame(X['AF4'].tolist())\n",
    "XFC5 = pd.DataFrame(X['FC5'].tolist())\n",
    "XFC6 = pd.DataFrame(X['FC6'].tolist())\n",
    "XP7 = pd.DataFrame(X['P7'].tolist())\n",
    "XP8 = pd.DataFrame(X['P8'].tolist())\n",
    "\n",
    "XWide = pd.concat([XO1, XO2, XF3, XF4, XF7, XF8, XT7, XT8, XAF3,\n",
    "                    XAF4, XFC5, XFC6, XP7, XP8], axis=1)\n",
    "del X\n",
    "del XO1\n",
    "del XO2\n",
    "del XF3\n",
    "del XF4\n",
    "del XF7\n",
    "del XF8\n",
    "del XT7\n",
    "del XT8\n",
    "del XAF3\n",
    "del XAF4\n",
    "del XFC5\n",
    "del XFC6\n",
    "del XP7\n",
    "del XP8\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New train-test split using XWide. \n",
    "\n",
    "XTrainWide, XTestWide, yTrain, yTest = train_test_split(XWide, y)\n",
    "\n",
    "# Create a simple Support Vector Classification as an alternative model\n",
    "# The following code takes a long time to run (~6 hrs on my computer)\n",
    "svc = SVC()\n",
    "svc.fit(XTrainWide, yTrain)\n",
    "\n",
    "svc.score(XTestWide, yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS\n",
    "---------------\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first iterations of the model were using data that was averaged per event. This effectively erased the time data, which is apparently crucial for neural coding (see this [video lecture by Earl Miller for more information on why](https://www.youtube.com/watch?v=Kqyhr9fTUjs)). This seems like it would be made worse by raw data being a measurement of changes in amlitude over time, even with filters. It might perform better the frequency information from Fourier transformation, but mean freqency without time would still lose a lot of information, other than perhaps the dominant frequency band of the channel. \n",
    "\n",
    "\n",
    ">\n",
    ">Best accuracy for the unprocessed signal with all channels was about .1020\n",
    "> &nbsp;\n",
    ">\n",
    ">In 6/14 channels, adding SGF data was about the same\n",
    ">\n",
    "> &nbsp;\n",
    ">\n",
    ">In 6/14 channels, adding SGF and dropping raw data resulted in really low accuracy and NaN loss\n",
    ">\n",
    "> &nbsp;\n",
    ">\n",
    ">In 6/14 channels, ICA only had a accuracy of about .1000\n",
    ">\n",
    "> &nbsp;\n",
    ">\n",
    ">6/14 channels, unproccessed+SGF+ICA, 22,059 params: NaN loss again, what's wrong with the data? woe be the futility of mine ways\n",
    ">\n",
    "> &nbsp;\n",
    ">\n",
    ">6/14 channels, SGF only, 9,771 params, accuracy still around .1000\n",
    ">\n",
    "> &nbsp;\n",
    ">\n",
    ">6/14 channels, raw, 9,771 params, accuracy about .1000, so the .0020 increase was only from the extra channels. Processing doesn't seem to change the accuracy at all.\n",
    ">\n",
    "> &nbsp;\n",
    ">\n",
    " ----------\n",
    "\n",
    " &nbsp;\n",
    "\n",
    "\n",
    "Realizing the temporal coding is crucial, I ran the model using vector data rows. The first iterations of this process indicated that the TF CNN algorithm cannot run on matrices whos values are both scalars and vectors. To correct this, I attempted to expand the vectors into columns of the dataframes, but this ends up being too large for the CNN, which gives the following error:\n",
    "\n",
    "```\n",
    "ResourceExhaustedError: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[799129600,32] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:StatelessRandomUniformV2]\n",
    "```\n",
    "\n",
    "However, a Support Vector Classifier was able to run with this input. Unfortunately, it gave only a slightly better accuracy score of 0.112 (+0.01). This was the raw data, and it might improve with denoising and/or source separation techniques, but other projects on the same dataset seem to not get much higher than around 20% accuracy. Though this answered modeling questions and taught me a lot, perhaps a different dataset would be a better option to create a deployable model to be used for BCI applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIBLIOGRAPHY\n",
    "---------------\n",
    "---------------\n",
    "- \"Deep learning for electroencephalogram (EEG) classification tasks: a review\", Alexander Craik, et al, 2019, J. Neural Eng. 16 031001, doi:10.1088/1741-2552/ab0ab5\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- \"Denoising Source Separation\", Jaako Särelä & Harri Valpola, 2005, J. Machine Learning Res. 6, pp. 233-272, doi:10.5555/1046920.1058110\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- \"Frequency Band and PCA Feature Comparison for EEG Signal Classification\", I Wayan Pio Pratama, et al, 2021, Lontar Komputer Vol. 12 No. 1, doi:10.24843/LKJITI.2021.v12.i01.p01\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- \"PIEEG: Turn a Raspberry Pi into a Brain-Computer-Interface to measure biosignals\", Ildar Rakhmatulin & Sebastian Volkl, 2022, arxiv::2201.02228\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- \"Progress in Brain Computer Interface: Challenges and Opportunities\", Simanto Saha, et al, 2021, Front. Syst. Neurosci., doi:10.3389/fnsys.2021.578875\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- \"Supply and demand analysis of the current and future US neurology workforce\", Timothy M. Dall, et al, 2013, Neurology 81(5), doi:10.1212/WNL.0b013e318294b1cf\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- \"Toward Direct Brain-Computer Communication\", Jacques J. Vidal, 1973, Ann. Rev Biophysics & Bioengineering, Vol. 2, pp. 157-180, doi:10.1146/annurev.bb.02.060173.001105\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- \"What is a Savitzky-Golay Filter?\", Ronald W. Schafer, 2011, IEEE Sig. Proc. Mag July 2011, pp. 111-117, doi:10.1109/MSP.2011.941097\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- Github repo listing many public EEG Datasets, including the one used in this project  https://github.com/meagmohit/EEG-Datasets\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- List of papers that use/reference the MindBigData digits dataset on its website http://mindbigdata.com/opendb/index.html\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- \"meegkit: EEG and MEG denoising in Python\" https://nbara.github.io/python-meegkit/ -- Unused but interesting EEG/MEG specific library\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- \"MNE, MEG + EEG Analysis & Visualization\"  https://mne.tools/dev/index.html -- Another, much larger, neuro signal analysis library\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- SciPy's signal processing library documentation https://docs.scipy.org/doc/scipy/reference/signal.html\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "- Website for Emotiv EPOC headset https://www.emotiv.com/epoc/ \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
